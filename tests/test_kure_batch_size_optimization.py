#!/usr/bin/env python3
"""
KURE v1 ëª¨ë¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸
ë‹¤ì–‘í•œ ë°°ì¹˜ ì‚¬ì´ì¦ˆì—ì„œ KURE v1 ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ ìµœì ê°’ì„ ì°¾ìŠµë‹ˆë‹¤.
"""

import os
import time
import json
import requests
import numpy as np
from typing import List, Dict, Any, Tuple
from datetime import datetime
from dotenv import load_dotenv
import statistics


# Load environment variables
load_dotenv()


class KureBatchSizeOptimizer:
    """KURE v1 ëª¨ë¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í´ë˜ìŠ¤"""
    
    def __init__(self):
        # API ì„¤ì •
        self.kure_api_key = os.getenv("API_KEY", "sk-kure-v1-test-key-12345")
        self.kure_base_url = os.getenv("BASE_URL", "http://localhost:8000")
        
        # KURE API í—¤ë”
        self.kure_headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.kure_api_key}"
        }
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.results = {}
        self.test_timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
        # í…ŒìŠ¤íŠ¸í•  ë°°ì¹˜ ì‚¬ì´ì¦ˆë“¤ (ìµœëŒ€ ì œí•œ ì°¾ê¸° ìœ„í•´ 150ê¹Œì§€)
        self.batch_sizes = [10,  32]
        
    def load_document(self, file_path: str) -> str:
        """ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(content)} ë¬¸ì")
            return content
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_document(self, text: str, chunk_size: int = 380, overlap: int = 70) -> List[str]:
        """KURE APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤."""
        try:
            payload = {
                "text": text,
                "strategy": "recursive",
                "chunk_size": chunk_size,
                "overlap": overlap,
                "language": "ko"
            }
            
            response = requests.post(
                f"{self.kure_base_url}/v1/chunk",
                json=payload,
                headers=self.kure_headers
            )
            
            if response.status_code == 200:
                data = response.json()
                chunks = [chunk['text'] for chunk in data['data']]
                print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                return chunks
            else:
                print(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì˜¤ë¥˜: {e}")
            return []

    def test_batch_size(self, chunks: List[str], batch_size: int, test_chunks: int = 500) -> Dict[str, Any]:
        """íŠ¹ì • ë°°ì¹˜ ì‚¬ì´ì¦ˆë¡œ KURE ì„ë² ë”© ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” ë°°ì¹˜ ì‚¬ì´ì¦ˆ {batch_size} í…ŒìŠ¤íŠ¸ (ì²­í¬ {test_chunks}ê°œ)...")
        
        # í…ŒìŠ¤íŠ¸ìš© ì²­í¬ ìƒ˜í”Œë§
        test_sample = chunks[:test_chunks] if len(chunks) >= test_chunks else chunks
        
        start_time = time.time()
        embeddings = []
        batch_times = []
        request_count = 0
        
        try:
            for i in range(0, len(test_sample), batch_size):
                batch = test_sample[i:i + batch_size]
                batch_start = time.time()
                
                payload = {
                    "input": batch,
                    "model": "nlpai-lab/KURE-v1"
                }
                
                response = requests.post(
                    f"{self.kure_base_url}/embeddings",
                    json=payload,
                    headers=self.kure_headers
                )
                
                batch_end = time.time()
                batch_time = batch_end - batch_start
                batch_times.append(batch_time)
                request_count += 1
                
                if response.status_code == 200:
                    data = response.json()
                    batch_embeddings = [item['embedding'] for item in data['data']]
                    embeddings.extend(batch_embeddings)
                    
                    if i % (batch_size * 10) == 0:  # ì§„í–‰ìƒí™© ì¶œë ¥
                        print(f"   ì§„í–‰: {len(embeddings)}/{len(test_sample)} ì²­í¬ ì™„ë£Œ")
                else:
                    print(f"âŒ ë°°ì¹˜ ì‹¤íŒ¨: {response.status_code}")
                    return {"success": False, "error": f"HTTP {response.status_code}"}
        
        except Exception as e:
            print(f"âŒ ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}
        
        total_time = time.time() - start_time
        
        result = {
            "success": True,
            "batch_size": batch_size,
            "test_chunks": len(test_sample),
            "total_time": total_time,
            "avg_time_per_chunk": total_time / len(test_sample),
            "chunks_per_second": len(test_sample) / total_time,
            "request_count": request_count,
            "avg_batch_time": statistics.mean(batch_times),
            "batch_time_std": statistics.stdev(batch_times) if len(batch_times) > 1 else 0,
            "min_batch_time": min(batch_times),
            "max_batch_time": max(batch_times),
            "throughput_per_request": len(test_sample) / request_count,
            "requests_per_second": request_count / total_time
        }
        
        print(f"âœ… ë°°ì¹˜ ì‚¬ì´ì¦ˆ {batch_size} ì™„ë£Œ:")
        print(f"   ì´ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ì²˜ë¦¬ ì†ë„: {result['chunks_per_second']:.2f} ì²­í¬/ì´ˆ")
        print(f"   ìš”ì²­ ìˆ˜: {request_count}ê°œ")
        print(f"   í‰ê·  ë°°ì¹˜ ì‹œê°„: {result['avg_batch_time']:.3f}ì´ˆ")
        
        return result

    def run_batch_size_optimization(self, chunks: List[str]) -> Dict[str, Any]:
        """ëª¨ë“  ë°°ì¹˜ ì‚¬ì´ì¦ˆì— ëŒ€í•´ ìµœì í™” í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print(f"\nğŸš€ KURE v1 ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print(f"ğŸ“„ í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {self.batch_sizes}")
        print("=" * 80)
        
        results = {}
        
        for batch_size in self.batch_sizes:
            result = self.test_batch_size(chunks, batch_size)
            if result["success"]:
                results[f"batch_{batch_size}"] = result
            else:
                print(f"âŒ ë°°ì¹˜ ì‚¬ì´ì¦ˆ {batch_size} í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
        
        return results

    def analyze_results(self, results: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        print(f"\nğŸ“Š ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” ê²°ê³¼ ë¶„ì„...")
        
        if not results:
            return {}
        
        # ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì¶œ
        batch_sizes = []
        chunks_per_second = []
        avg_batch_times = []
        requests_per_second = []
        
        for key, result in results.items():
            batch_sizes.append(result["batch_size"])
            chunks_per_second.append(result["chunks_per_second"])
            avg_batch_times.append(result["avg_batch_time"])
            requests_per_second.append(result["requests_per_second"])
        
        # ìµœì ê°’ ì°¾ê¸°
        max_throughput_idx = chunks_per_second.index(max(chunks_per_second))
        optimal_batch_size = batch_sizes[max_throughput_idx]
        max_throughput = chunks_per_second[max_throughput_idx]
        
        # ë¶„ì„ ê²°ê³¼
        analysis = {
            "optimal_batch_size": optimal_batch_size,
            "max_throughput": max_throughput,
            "performance_summary": {
                "batch_sizes": batch_sizes,
                "chunks_per_second": chunks_per_second,
                "avg_batch_times": avg_batch_times,
                "requests_per_second": requests_per_second
            },
            "recommendations": self.generate_recommendations(results)
        }
        
        print(f"ğŸ† ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {optimal_batch_size}")
        print(f"ğŸ† ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {max_throughput:.2f} ì²­í¬/ì´ˆ")
        
        return analysis

    def generate_recommendations(self, results: Dict[str, Any]) -> List[str]:
        """ì„±ëŠ¥ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê¶Œì¥ì‚¬í•­ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        recommendations = []

        # ì²˜ë¦¬ëŸ‰ ê¸°ì¤€ ìƒìœ„ 3ê°œ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ì°¾ê¸°
        sorted_results = sorted(results.items(),
                              key=lambda x: x[1]["chunks_per_second"],
                              reverse=True)

        top_3 = sorted_results[:3]

        recommendations.append(f"ìµœê³  ì„±ëŠ¥: ë°°ì¹˜ ì‚¬ì´ì¦ˆ {top_3[0][1]['batch_size']} ({top_3[0][1]['chunks_per_second']:.2f} ì²­í¬/ì´ˆ)")

        if len(top_3) > 1:
            recommendations.append(f"2ìœ„: ë°°ì¹˜ ì‚¬ì´ì¦ˆ {top_3[1][1]['batch_size']} ({top_3[1][1]['chunks_per_second']:.2f} ì²­í¬/ì´ˆ)")

        if len(top_3) > 2:
            recommendations.append(f"3ìœ„: ë°°ì¹˜ ì‚¬ì´ì¦ˆ {top_3[2][1]['batch_size']} ({top_3[2][1]['chunks_per_second']:.2f} ì²­í¬/ì´ˆ)")

        # ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± ê³ ë ¤
        memory_efficient = [r for r in sorted_results if r[1]["batch_size"] <= 32]
        if memory_efficient:
            best_small = memory_efficient[0]
            recommendations.append(f"ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ì„ íƒ: ë°°ì¹˜ ì‚¬ì´ì¦ˆ {best_small[1]['batch_size']} ({best_small[1]['chunks_per_second']:.2f} ì²­í¬/ì´ˆ)")

        # ì•ˆì •ì„± ê³ ë ¤ (ë°°ì¹˜ ì‹œê°„ í‘œì¤€í¸ì°¨ê°€ ë‚®ì€ ê²ƒ)
        stable_results = sorted(results.items(), key=lambda x: x[1]["batch_time_std"])
        if stable_results:
            most_stable = stable_results[0]
            recommendations.append(f"ê°€ì¥ ì•ˆì •ì : ë°°ì¹˜ ì‚¬ì´ì¦ˆ {most_stable[1]['batch_size']} (í‘œì¤€í¸ì°¨: {most_stable[1]['batch_time_std']:.3f}ì´ˆ)")

        return recommendations



    def save_results(self, results: Dict[str, Any], analysis: Dict[str, Any]) -> str:
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = f"test_outputs/{self.test_timestamp}"
        os.makedirs(output_dir, exist_ok=True)

        # ì „ì²´ ê²°ê³¼ JSON ì €ì¥
        full_report = {
            "test_info": {
                "timestamp": self.test_timestamp,
                "batch_sizes_tested": self.batch_sizes,
                "test_type": "KURE v1 Batch Size Optimization"
            },
            "detailed_results": results,
            "analysis": analysis
        }

        json_file = f"{output_dir}/kure_batch_size_optimization.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(full_report, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_file = f"{output_dir}/batch_size_summary.md"
        self.generate_markdown_report(full_report, summary_file)

        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   ğŸ“„ JSON ë¦¬í¬íŠ¸: {json_file}")
        print(f"   ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")

        return output_dir

    def generate_markdown_report(self, report: Dict[str, Any], file_path: str):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# KURE v1 ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” ë¦¬í¬íŠ¸\n\n")
            f.write(f"**í…ŒìŠ¤íŠ¸ ì‹œê°„**: {report['test_info']['timestamp']}\n")
            f.write(f"**í…ŒìŠ¤íŠ¸ ë°°ì¹˜ ì‚¬ì´ì¦ˆ**: {', '.join(map(str, report['test_info']['batch_sizes_tested']))}\n\n")

            # ìµœì í™” ê²°ê³¼
            analysis = report["analysis"]
            f.write("## ğŸ† ìµœì í™” ê²°ê³¼\n\n")
            f.write(f"**ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ**: {analysis['optimal_batch_size']}\n")
            f.write(f"**ìµœëŒ€ ì²˜ë¦¬ëŸ‰**: {analysis['max_throughput']:.2f} ì²­í¬/ì´ˆ\n\n")

            # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
            f.write("## ğŸ“Š ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write("| ë°°ì¹˜ ì‚¬ì´ì¦ˆ | ì²˜ë¦¬ëŸ‰ (ì²­í¬/ì´ˆ) | í‰ê·  ë°°ì¹˜ ì‹œê°„ (ì´ˆ) | ìš”ì²­/ì´ˆ | í‘œì¤€í¸ì°¨ |\n")
            f.write("|-------------|------------------|-------------------|---------|----------|\n")

            for result in report["detailed_results"].values():
                f.write(f"| {result['batch_size']} | {result['chunks_per_second']:.2f} | {result['avg_batch_time']:.3f} | {result['requests_per_second']:.2f} | {result['batch_time_std']:.3f} |\n")

            # ê¶Œì¥ì‚¬í•­
            f.write("\n## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n")
            for i, rec in enumerate(analysis["recommendations"], 1):
                f.write(f"{i}. {rec}\n")

    def run_full_optimization(self, document_path: str) -> str:
        """ì „ì²´ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸ§ª KURE v1 ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)

        # 1. ë¬¸ì„œ ë¡œë“œ
        document_content = self.load_document(document_path)
        if not document_content:
            print("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
            return ""

        # 2. ë¬¸ì„œ ì²­í‚¹
        chunks = self.chunk_document(document_content)
        if not chunks:
            print("âŒ ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨")
            return ""

        # 3. ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸
        results = self.run_batch_size_optimization(chunks)
        if not results:
            print("âŒ ë°°ì¹˜ ì‚¬ì´ì¦ˆ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return ""

        # 4. ê²°ê³¼ ë¶„ì„
        analysis = self.analyze_results(results)

        # 5. ê²°ê³¼ ì €ì¥
        output_dir = self.save_results(results, analysis)

        # 6. ê²°ê³¼ ì¶œë ¥
        self.print_summary(results, analysis)

        return output_dir

    def print_summary(self, results: Dict[str, Any], analysis: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ‰ ë°°ì¹˜ ì‚¬ì´ì¦ˆ ìµœì í™” í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 80)

        print(f"\nğŸ† ìµœì í™” ê²°ê³¼:")
        print(f"  ìµœì  ë°°ì¹˜ ì‚¬ì´ì¦ˆ: {analysis['optimal_batch_size']}")
        print(f"  ìµœëŒ€ ì²˜ë¦¬ëŸ‰: {analysis['max_throughput']:.2f} ì²­í¬/ì´ˆ")

        print(f"\nğŸ“Š ìƒìœ„ ì„±ëŠ¥ ë°°ì¹˜ ì‚¬ì´ì¦ˆ:")
        sorted_results = sorted(results.items(),
                              key=lambda x: x[1]["chunks_per_second"],
                              reverse=True)

        for i, (_, result) in enumerate(sorted_results[:5], 1):
            print(f"  {i}. ë°°ì¹˜ ì‚¬ì´ì¦ˆ {result['batch_size']}: {result['chunks_per_second']:.2f} ì²­í¬/ì´ˆ")

        print(f"\nğŸ’¡ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(analysis["recommendations"], 1):
            print(f"  {i}. {rec}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë¬¸ì„œ ê²½ë¡œ
    document_path = "sample_docs/P02_01_01_001_20210101_marker.md"

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    optimizer = KureBatchSizeOptimizer()
    output_dir = optimizer.run_full_optimization(document_path)

    if output_dir:
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
