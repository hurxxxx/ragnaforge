#!/usr/bin/env python3
"""
.env ì„¤ì •ì„ ì‚¬ìš©í•œ ë¶€ë¶„ ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸ (ì•ˆì •ì„±ì„ ìœ„í•´)
"""

import requests
import json
import os
import time
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
BASE_URL = "http://localhost:8000"
API_KEY = "sk-kure-v1-test-key-12345"

# .envì—ì„œ ì²­í‚¹ ì„¤ì • ì½ê¸°
DEFAULT_CHUNK_STRATEGY = os.getenv("DEFAULT_CHUNK_STRATEGY", "recursive")
DEFAULT_CHUNK_SIZE = int(os.getenv("DEFAULT_CHUNK_SIZE", "380"))
DEFAULT_CHUNK_OVERLAP = int(os.getenv("DEFAULT_CHUNK_OVERLAP", "70"))
DEFAULT_CHUNK_LANGUAGE = os.getenv("DEFAULT_CHUNK_LANGUAGE", "auto")

def main():
    print("ğŸ§ª .env ì„¤ì •ì„ ì‚¬ìš©í•œ ì‹¤ì œ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ (ë¶€ë¶„ ì²˜ë¦¬)")
    print("=" * 70)
    print(f"ğŸ“‹ .env ì²­í‚¹ ì„¤ì •:")
    print(f"   ì „ëµ: {DEFAULT_CHUNK_STRATEGY}")
    print(f"   ì²­í¬ í¬ê¸°: {DEFAULT_CHUNK_SIZE}")
    print(f"   ì˜¤ë²„ë©: {DEFAULT_CHUNK_OVERLAP}")
    print(f"   ì–¸ì–´: {DEFAULT_CHUNK_LANGUAGE}")
    
    # 1. ì „ì²´ ë¬¸ì„œ ë¡œë“œ
    doc_path = "sample_docs/ê¸°ì—… ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ë¶„ì„.md"
    
    if not os.path.exists(doc_path):
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_path}")
        return
    
    with open(doc_path, 'r', encoding='utf-8') as f:
        full_content = f.read()
    
    # ì•ˆì •ì„±ì„ ìœ„í•´ ë¬¸ì„œì˜ ì²˜ìŒ 10,000ìë§Œ ì‚¬ìš©
    content = full_content[:10000]
    
    print(f"\nğŸ“„ ë¬¸ì„œ ë¡œë“œ:")
    print(f"   ğŸ“ ì „ì²´ ë¬¸ì„œ: {len(full_content):,} ë¬¸ì")
    print(f"   ğŸ“ í…ŒìŠ¤íŠ¸ ë¶€ë¶„: {len(content):,} ë¬¸ì (ì²˜ìŒ 10,000ì)")
    print(f"   ğŸ“Š í…ŒìŠ¤íŠ¸ ë‹¨ì–´ ìˆ˜: {len(content.split()):,}")
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    # 2. .env ì„¤ì •ìœ¼ë¡œ ì²­í‚¹
    print(f"\nğŸ”ª ë¬¸ì„œ ì²­í‚¹ (.env ì„¤ì • ì‚¬ìš©)...")
    
    chunk_payload = {
        "text": content,
        "strategy": DEFAULT_CHUNK_STRATEGY,
        "chunk_size": DEFAULT_CHUNK_SIZE,
        "overlap": DEFAULT_CHUNK_OVERLAP,
        "language": DEFAULT_CHUNK_LANGUAGE
    }
    
    start_time = time.time()
    response = requests.post(f"{BASE_URL}/v1/chunk", headers=headers, json=chunk_payload)
    chunk_time = time.time() - start_time
    
    if response.status_code == 200:
        chunk_result = response.json()
        chunks = chunk_result["data"]
        
        print(f"âœ… ì²­í‚¹ ì„±ê³µ!")
        print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {chunk_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(chunks):,}")
        print(f"   ğŸ”¢ ì´ í† í° ìˆ˜: {chunk_result['total_tokens']:,}")
        print(f"   ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°: {chunk_result['total_tokens'] / len(chunks):.1f} í† í°")
        print(f"   ğŸ“ˆ ì••ì¶•ë¥ : {chunk_result['original_length'] / chunk_result['total_tokens']:.2f} ë¬¸ì/í† í°")
        
        # ì²­í¬ í¬ê¸° ë¶„í¬
        token_counts = [chunk['token_count'] for chunk in chunks]
        min_tokens = min(token_counts)
        max_tokens = max(token_counts)
        print(f"   ğŸ“‰ í† í° ë²”ìœ„: {min_tokens} ~ {max_tokens}")
        
        # ëª¨ë“  ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“ ëª¨ë“  ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:")
        for i, chunk in enumerate(chunks):
            preview = chunk['text'][:80].replace('\n', ' ') + "..."
            print(f"   ì²­í¬ {i+1}: {chunk['token_count']} í† í°")
            print(f"      {preview}")
        
        # 3. ëª¨ë“  ì²­í¬ ì„ë² ë”©
        print(f"\nğŸ§  ëª¨ë“  ì²­í¬ ì„ë² ë”© ìƒì„±...")
        
        chunk_texts = [chunk['text'] for chunk in chunks]
        
        embed_payload = {
            "input": chunk_texts,
            "model": "nlpai-lab/KURE-v1"
        }
        
        start_time = time.time()
        response = requests.post(f"{BASE_URL}/embeddings", headers=headers, json=embed_payload)
        embed_time = time.time() - start_time
        
        if response.status_code == 200:
            embed_result = response.json()
            embeddings = [data['embedding'] for data in embed_result['data']]
            
            embedding_dim = len(embeddings[0])
            total_tokens_used = embed_result['usage']['total_tokens']
            tokens_per_second = total_tokens_used / embed_time if embed_time > 0 else 0
            
            print(f"âœ… ì„ë² ë”© ìƒì„± ì„±ê³µ!")
            print(f"   âœ… ì´ ì„ë² ë”© ìˆ˜: {len(embeddings):,}")
            print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {embedding_dim:,}")
            print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {embed_time:.2f}ì´ˆ")
            print(f"   ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {total_tokens_used:,}")
            print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ")
            
            # ì„ë² ë”© í†µê³„
            print(f"\nğŸ“Š ì„ë² ë”© í†µê³„:")
            for i, embedding in enumerate(embeddings):
                avg_val = sum(embedding) / len(embedding)
                min_val = min(embedding)
                max_val = max(embedding)
                norm = (sum(x*x for x in embedding)) ** 0.5
                print(f"   ì„ë² ë”© {i+1}: í‰ê· ={avg_val:.4f}, ë²”ìœ„=[{min_val:.4f}, {max_val:.4f}], ë…¸ë¦„={norm:.4f}")
            
            # 4. ìœ ì‚¬ë„ ë¶„ì„
            print(f"\nğŸ”„ ìœ ì‚¬ë„ ë¶„ì„...")
            
            sim_payload = {
                "texts": chunk_texts,
                "model": "nlpai-lab/KURE-v1"
            }
            
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/similarity", headers=headers, json=sim_payload)
            sim_time = time.time() - start_time
            
            if response.status_code == 200:
                sim_result = response.json()
                similarities = sim_result['similarities']
                
                print(f"âœ… ìœ ì‚¬ë„ ê³„ì‚° ì„±ê³µ!")
                print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {sim_time:.2f}ì´ˆ")
                print(f"   ğŸ“Š í–‰ë ¬ í¬ê¸°: {len(similarities)}x{len(similarities[0])}")
                
                # ìœ ì‚¬ë„ í–‰ë ¬ ì¶œë ¥
                print(f"\nğŸ“ˆ ìœ ì‚¬ë„ í–‰ë ¬:")
                print("     ", end="")
                for i in range(len(similarities)):
                    print(f"ì²­í¬{i+1:2d}", end="  ")
                print()
                
                for i, row in enumerate(similarities):
                    print(f"ì²­í¬{i+1:2d}", end="  ")
                    for val in row:
                        print(f"{val:.3f}", end="  ")
                    print()
                
                # ìœ ì‚¬ë„ í†µê³„
                all_similarities = []
                for i in range(len(similarities)):
                    for j in range(i+1, len(similarities[i])):
                        all_similarities.append(similarities[i][j])
                
                if all_similarities:
                    avg_sim = sum(all_similarities) / len(all_similarities)
                    min_sim = min(all_similarities)
                    max_sim = max(all_similarities)
                    
                    print(f"\nğŸ“ˆ ìœ ì‚¬ë„ í†µê³„:")
                    print(f"   í‰ê·  ìœ ì‚¬ë„: {avg_sim:.4f}")
                    print(f"   ìµœì†Œ ìœ ì‚¬ë„: {min_sim:.4f}")
                    print(f"   ìµœëŒ€ ìœ ì‚¬ë„: {max_sim:.4f}")
                    
                    # ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ
                    max_similarity = 0
                    max_pair = (0, 0)
                    for i in range(len(similarities)):
                        for j in range(i+1, len(similarities[i])):
                            if similarities[i][j] > max_similarity:
                                max_similarity = similarities[i][j]
                                max_pair = (i, j)
                    
                    print(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ:")
                    print(f"   ì²­í¬ {max_pair[0]+1} â†” ì²­í¬ {max_pair[1]+1} (ìœ ì‚¬ë„: {max_similarity:.4f})")
                    print(f"   ì²­í¬ {max_pair[0]+1}: {chunks[max_pair[0]]['text'][:100]}...")
                    print(f"   ì²­í¬ {max_pair[1]+1}: {chunks[max_pair[1]]['text'][:100]}...")
            else:
                print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {response.status_code}")
                print(f"   ì˜¤ë¥˜: {response.text}")
        else:
            print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨: {response.status_code}")
            print(f"   ì˜¤ë¥˜: {response.text}")
        
        # 5. ì „ì²´ ì²˜ë¦¬ ìš”ì•½
        total_time = chunk_time + embed_time
        print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ìš”ì•½:")
        print(f"   ğŸ“„ ì²˜ë¦¬ëœ ë¬¸ì„œ í¬ê¸°: {len(content):,} ë¬¸ì")
        print(f"   ğŸ”ª ì²­í‚¹ ì‹œê°„: {chunk_time:.2f}ì´ˆ")
        print(f"   ğŸ§  ì„ë² ë”© ì‹œê°„: {embed_time:.2f}ì´ˆ")
        print(f"   â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ìƒì„±ëœ ì²­í¬ ìˆ˜: {len(chunks):,}")
        print(f"   ğŸ”¢ ì‚¬ìš©ëœ í† í°: {total_tokens_used:,}")
        print(f"   âš¡ ì „ì²´ ì²˜ë¦¬ ì†ë„: {len(content) / total_time:.1f} ë¬¸ì/ì´ˆ")
        print(f"   ğŸ¯ .env ì„¤ì • ì ìš©: âœ…")
        
        print(f"\nğŸ‰ .env ì„¤ì • ê¸°ë°˜ ì‹¤ì œ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    else:
        print(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {response.status_code}")
        print(f"   ì˜¤ë¥˜: {response.text}")

if __name__ == "__main__":
    main()
