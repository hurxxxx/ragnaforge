#!/usr/bin/env python3
"""
ì „ì²´ ë¬¸ì„œ íŒŒì¼ì„ ì‚¬ìš©í•œ ì™„ì „í•œ ì²­í‚¹ ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸
"""

import requests
import json
import os
import time
import math
from typing import List, Dict, Any
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
BASE_URL = "http://localhost:8000"
API_KEY = "sk-kure-v1-test-key-12345"

# .envì—ì„œ ì²­í‚¹ ì„¤ì • ì½ê¸°
DEFAULT_CHUNK_STRATEGY = os.getenv("DEFAULT_CHUNK_STRATEGY", "recursive")
DEFAULT_CHUNK_SIZE = int(os.getenv("DEFAULT_CHUNK_SIZE", "380"))
DEFAULT_CHUNK_OVERLAP = int(os.getenv("DEFAULT_CHUNK_OVERLAP", "70"))
DEFAULT_CHUNK_LANGUAGE = os.getenv("DEFAULT_CHUNK_LANGUAGE", "auto")

def load_complete_document() -> str:
    """ì „ì²´ ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    doc_path = "sample_docs/ê¸°ì—… ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ë¶„ì„.md"
    
    if not os.path.exists(doc_path):
        raise FileNotFoundError(f"ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_path}")
    
    with open(doc_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"ğŸ“„ ì „ì²´ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
    print(f"   ğŸ“ ì´ ë¬¸ì ìˆ˜: {len(content):,}")
    print(f"   ğŸ“ ì´ ì¤„ ìˆ˜: {content.count(chr(10)) + 1:,}")
    print(f"   ğŸ“Š ë‹¨ì–´ ìˆ˜ (ì¶”ì •): {len(content.split()):,}")
    
    return content

def test_complete_document_chunking(text: str) -> Dict[str, List[Dict[str, Any]]]:
    """ì „ì²´ ë¬¸ì„œë¥¼ .env ì„¤ì •ì„ ì‚¬ìš©í•˜ì—¬ ì²­í‚¹í•©ë‹ˆë‹¤."""
    print(f"\nğŸ”ª ì „ì²´ ë¬¸ì„œ ì²­í‚¹ í…ŒìŠ¤íŠ¸ (.env ì„¤ì • ì‚¬ìš©)")
    print("=" * 80)
    print(f"ğŸ“‹ .env ì²­í‚¹ ì„¤ì •:")
    print(f"   ì „ëµ: {DEFAULT_CHUNK_STRATEGY}")
    print(f"   ì²­í¬ í¬ê¸°: {DEFAULT_CHUNK_SIZE}")
    print(f"   ì˜¤ë²„ë©: {DEFAULT_CHUNK_OVERLAP}")
    print(f"   ì–¸ì–´: {DEFAULT_CHUNK_LANGUAGE}")

    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }

    # .env ì„¤ì •ì„ ê¸°ë³¸ìœ¼ë¡œ í•˜ê³  ë¹„êµë¥¼ ìœ„í•´ ë‹¤ë¥¸ ì „ëµë“¤ë„ í…ŒìŠ¤íŠ¸
    strategies = [DEFAULT_CHUNK_STRATEGY, "sentence", "token"]
    # ì¤‘ë³µ ì œê±°
    strategies = list(dict.fromkeys(strategies))
    all_chunks = {}

    for strategy in strategies:
        print(f"\nğŸ“‹ ì „ëµ: {strategy}")
        print("-" * 40)
        
        # .env ì„¤ì •ì„ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©í•˜ë˜, ì „ëµë§Œ ë³€ê²½
        payload = {
            "text": text,
            "strategy": strategy,
            "chunk_size": DEFAULT_CHUNK_SIZE,
            "overlap": DEFAULT_CHUNK_OVERLAP,
            "language": DEFAULT_CHUNK_LANGUAGE
        }
        
        start_time = time.time()
        response = requests.post(f"{BASE_URL}/v1/chunk", headers=headers, json=payload)
        end_time = time.time()
        
        if response.status_code == 200:
            result = response.json()
            chunks = result["data"]
            all_chunks[strategy] = chunks
            
            processing_time = end_time - start_time
            avg_chunk_size = result['total_tokens'] / len(chunks) if chunks else 0
            
            print(f"âœ… ì²­í‚¹ ì„±ê³µ!")
            print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {processing_time:.2f}ì´ˆ")
            print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(chunks):,}")
            print(f"   ğŸ”¢ ì´ í† í° ìˆ˜: {result['total_tokens']:,}")
            print(f"   ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°: {avg_chunk_size:.1f} í† í°")
            print(f"   ğŸ“ˆ ì••ì¶•ë¥ : {result['original_length'] / result['total_tokens']:.2f} ë¬¸ì/í† í°")
            
            # ì²­í¬ í¬ê¸° ë¶„í¬ ë¶„ì„
            token_counts = [chunk['token_count'] for chunk in chunks]
            min_tokens = min(token_counts)
            max_tokens = max(token_counts)
            
            print(f"   ğŸ“‰ í† í° ë²”ìœ„: {min_tokens} ~ {max_tokens}")
            
            # ì²˜ìŒ 3ê°œì™€ ë§ˆì§€ë§‰ 3ê°œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
            print(f"\n   ğŸ“ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°:")
            for i, chunk in enumerate(chunks[:3]):
                preview = chunk['text'][:80].replace('\n', ' ') + "..."
                print(f"      ì²­í¬ {i+1}: {chunk['token_count']} í† í° - {preview}")
            
            if len(chunks) > 6:
                print(f"      ... ({len(chunks)-6}ê°œ ì²­í¬ ìƒëµ) ...")
                for i, chunk in enumerate(chunks[-3:], len(chunks)-2):
                    preview = chunk['text'][:80].replace('\n', ' ') + "..."
                    print(f"      ì²­í¬ {i}: {chunk['token_count']} í† í° - {preview}")
        else:
            print(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {response.status_code}")
            print(f"   ì˜¤ë¥˜: {response.text}")
    
    return all_chunks

def test_complete_embeddings(chunks: List[Dict[str, Any]], strategy_name: str, batch_size: int = 8) -> List[List[float]]:
    """ì „ì²´ ì²­í¬ë“¤ì˜ ì„ë² ë”©ì„ ë°°ì¹˜ë¡œ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"\nğŸ§  ì „ì²´ ì„ë² ë”© ìƒì„± í…ŒìŠ¤íŠ¸ ({strategy_name} ì „ëµ)")
    print("=" * 80)
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    chunk_texts = [chunk['text'] for chunk in chunks]
    total_chunks = len(chunk_texts)
    total_batches = math.ceil(total_chunks / batch_size)
    
    print(f"ğŸ“Š ì„ë² ë”© ì²˜ë¦¬ ì •ë³´:")
    print(f"   ğŸ“ ì´ ì²­í¬ ìˆ˜: {total_chunks:,}")
    print(f"   ğŸ“¦ ë°°ì¹˜ í¬ê¸°: {batch_size}")
    print(f"   ğŸ”„ ì´ ë°°ì¹˜ ìˆ˜: {total_batches:,}")
    
    all_embeddings = []
    total_tokens_used = 0
    total_processing_time = 0
    
    for batch_idx in range(total_batches):
        start_idx = batch_idx * batch_size
        end_idx = min(start_idx + batch_size, total_chunks)
        batch_texts = chunk_texts[start_idx:end_idx]
        
        print(f"\nğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_texts)}ê°œ ì²­í¬)")
        
        payload = {
            "input": batch_texts,
            "model": "nlpai-lab/KURE-v1"
        }
        
        start_time = time.time()
        response = requests.post(f"{BASE_URL}/embeddings", headers=headers, json=payload)
        end_time = time.time()
        
        batch_time = end_time - start_time
        total_processing_time += batch_time
        
        if response.status_code == 200:
            result = response.json()
            batch_embeddings = [data['embedding'] for data in result['data']]
            all_embeddings.extend(batch_embeddings)
            
            batch_tokens = result['usage']['total_tokens']
            total_tokens_used += batch_tokens
            
            print(f"   âœ… ì„±ê³µ: {len(batch_embeddings)}ê°œ ì„ë² ë”©, {batch_tokens} í† í°, {batch_time:.2f}ì´ˆ")
        else:
            print(f"   âŒ ì‹¤íŒ¨: {response.status_code} - {response.text}")
            return []
    
    if all_embeddings:
        embedding_dim = len(all_embeddings[0])
        avg_processing_time = total_processing_time / total_batches
        tokens_per_second = total_tokens_used / total_processing_time if total_processing_time > 0 else 0
        
        print(f"\nğŸ“ˆ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
        print(f"   âœ… ì´ ì„ë² ë”© ìˆ˜: {len(all_embeddings):,}")
        print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {embedding_dim:,}")
        print(f"   â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.2f}ì´ˆ")
        print(f"   ğŸš€ í‰ê·  ë°°ì¹˜ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        print(f"   ğŸ”¢ ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens_used:,}")
        print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ")
        
        # ì„ë² ë”© í†µê³„
        print(f"\nğŸ“Š ì„ë² ë”© í†µê³„ (ì²˜ìŒ 5ê°œ):")
        for i, embedding in enumerate(all_embeddings[:5]):
            avg_val = sum(embedding) / len(embedding)
            min_val = min(embedding)
            max_val = max(embedding)
            norm = math.sqrt(sum(x*x for x in embedding))
            print(f"   ì„ë² ë”© {i+1}: í‰ê· ={avg_val:.4f}, ë²”ìœ„=[{min_val:.4f}, {max_val:.4f}], ë…¸ë¦„={norm:.4f}")
    
    return all_embeddings

def test_similarity_analysis(chunks: List[Dict[str, Any]], embeddings: List[List[float]], strategy_name: str):
    """ì„ë² ë”©ì„ ì‚¬ìš©í•œ ìœ ì‚¬ë„ ë¶„ì„"""
    print(f"\nğŸ”„ ìœ ì‚¬ë„ ë¶„ì„ ({strategy_name} ì „ëµ)")
    print("=" * 80)
    
    if len(chunks) < 2:
        print("âŒ ìœ ì‚¬ë„ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ìµœì†Œ 2ê°œì˜ ì²­í¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        return
    
    # ì²˜ìŒ 10ê°œ ì²­í¬ë¡œ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
    sample_size = min(10, len(chunks))
    sample_chunks = chunks[:sample_size]
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    chunk_texts = [chunk['text'] for chunk in sample_chunks]
    
    payload = {
        "texts": chunk_texts,
        "model": "nlpai-lab/KURE-v1"
    }
    
    start_time = time.time()
    response = requests.post(f"{BASE_URL}/similarity", headers=headers, json=payload)
    end_time = time.time()
    
    if response.status_code == 200:
        result = response.json()
        similarity_matrix = result['similarities']
        
        print(f"âœ… ìœ ì‚¬ë„ ê³„ì‚° ì„±ê³µ!")
        print(f"   ğŸ“Š ë¶„ì„ ì²­í¬ ìˆ˜: {sample_size}")
        print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {end_time - start_time:.2f}ì´ˆ")
        print(f"   ğŸ¯ ì‚¬ìš© ëª¨ë¸: {result['model']}")
        
        # ìœ ì‚¬ë„ í†µê³„
        all_similarities = []
        for i in range(len(similarity_matrix)):
            for j in range(i+1, len(similarity_matrix[i])):
                all_similarities.append(similarity_matrix[i][j])
        
        if all_similarities:
            avg_sim = sum(all_similarities) / len(all_similarities)
            min_sim = min(all_similarities)
            max_sim = max(all_similarities)
            
            print(f"\nğŸ“ˆ ìœ ì‚¬ë„ í†µê³„:")
            print(f"   í‰ê·  ìœ ì‚¬ë„: {avg_sim:.4f}")
            print(f"   ìµœì†Œ ìœ ì‚¬ë„: {min_sim:.4f}")
            print(f"   ìµœëŒ€ ìœ ì‚¬ë„: {max_sim:.4f}")
            
            # ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ ì°¾ê¸°
            max_pair = (0, 0)
            max_similarity = 0
            for i in range(len(similarity_matrix)):
                for j in range(i+1, len(similarity_matrix[i])):
                    if similarity_matrix[i][j] > max_similarity:
                        max_similarity = similarity_matrix[i][j]
                        max_pair = (i, j)
            
            print(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ:")
            print(f"   ì²­í¬ {max_pair[0]+1} â†” ì²­í¬ {max_pair[1]+1} (ìœ ì‚¬ë„: {max_similarity:.4f})")
            print(f"   ì²­í¬ {max_pair[0]+1}: {sample_chunks[max_pair[0]]['text'][:100]}...")
            print(f"   ì²­í¬ {max_pair[1]+1}: {sample_chunks[max_pair[1]]['text'][:100]}...")
    else:
        print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {response.status_code}")
        print(f"   ì˜¤ë¥˜: {response.text}")

def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ í•¨ìˆ˜"""
    print("ğŸ§ª ì „ì²´ ë¬¸ì„œë¥¼ ì‚¬ìš©í•œ ì™„ì „í•œ ì²­í‚¹ ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸")
    print("=" * 100)
    print(f"ğŸ”‘ API í‚¤: {API_KEY[:20]}...")
    print(f"ğŸŒ ê¸°ë³¸ URL: {BASE_URL}")
    
    try:
        # 1. ì „ì²´ ë¬¸ì„œ ë¡œë“œ
        document_text = load_complete_document()
        
        # 2. ì „ì²´ ë¬¸ì„œ ì²­í‚¹ (ëª¨ë“  ì „ëµ)
        all_chunks = test_complete_document_chunking(document_text)
        
        # 3. .env ì„¤ì •ì˜ ê¸°ë³¸ ì „ëµìœ¼ë¡œ ì „ì²´ ì„ë² ë”© ìƒì„±
        if DEFAULT_CHUNK_STRATEGY in all_chunks:
            chunks = all_chunks[DEFAULT_CHUNK_STRATEGY]
            embeddings = test_complete_embeddings(chunks, DEFAULT_CHUNK_STRATEGY)

            # 4. ìœ ì‚¬ë„ ë¶„ì„
            if embeddings:
                test_similarity_analysis(chunks, embeddings, DEFAULT_CHUNK_STRATEGY)
        
        # 5. ì „ëµë³„ ì„±ëŠ¥ ë¹„êµ
        print(f"\nğŸ“Š ì „ëµë³„ ì²­í‚¹ ì„±ëŠ¥ ë¹„êµ")
        print("=" * 80)
        print(f"{'ì „ëµ':<12} {'ì²­í¬ìˆ˜':<8} {'í‰ê· í† í°':<10} {'íš¨ìœ¨ì„±':<10}")
        print("-" * 50)
        
        for strategy, chunks in all_chunks.items():
            if chunks:
                total_tokens = sum(chunk['token_count'] for chunk in chunks)
                avg_tokens = total_tokens / len(chunks)
                efficiency = len(document_text) / total_tokens  # ë¬¸ì/í† í° ë¹„ìœ¨
                print(f"{strategy:<12} {len(chunks):<8} {avg_tokens:<10.1f} {efficiency:<10.2f}")
        
        print(f"\nğŸ‰ ì „ì²´ ë¬¸ì„œ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except FileNotFoundError as e:
        print(f"âŒ íŒŒì¼ ì˜¤ë¥˜: {e}")
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
