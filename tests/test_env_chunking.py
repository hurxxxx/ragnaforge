#!/usr/bin/env python3
"""
.env ì„¤ì •ì„ ì‚¬ìš©í•œ ì „ì²´ ë¬¸ì„œ ì²­í‚¹ ë° ì„ë² ë”© í…ŒìŠ¤íŠ¸
"""

import requests
import json
import os
import time
from dotenv import load_dotenv

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

# API ì„¤ì •
BASE_URL = "http://localhost:8000"
API_KEY = "sk-kure-v1-test-key-12345"

# .envì—ì„œ ì²­í‚¹ ì„¤ì • ì½ê¸°
DEFAULT_CHUNK_STRATEGY = os.getenv("DEFAULT_CHUNK_STRATEGY", "recursive")
DEFAULT_CHUNK_SIZE = int(os.getenv("DEFAULT_CHUNK_SIZE", "380"))
DEFAULT_CHUNK_OVERLAP = int(os.getenv("DEFAULT_CHUNK_OVERLAP", "70"))
DEFAULT_CHUNK_LANGUAGE = os.getenv("DEFAULT_CHUNK_LANGUAGE", "auto")

def main():
    print("ğŸ§ª .env ì„¤ì •ì„ ì‚¬ìš©í•œ ì „ì²´ ë¬¸ì„œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    print(f"ğŸ“‹ .env ì²­í‚¹ ì„¤ì •:")
    print(f"   ì „ëµ: {DEFAULT_CHUNK_STRATEGY}")
    print(f"   ì²­í¬ í¬ê¸°: {DEFAULT_CHUNK_SIZE}")
    print(f"   ì˜¤ë²„ë©: {DEFAULT_CHUNK_OVERLAP}")
    print(f"   ì–¸ì–´: {DEFAULT_CHUNK_LANGUAGE}")
    
    # 1. ì „ì²´ ë¬¸ì„œ ë¡œë“œ
    doc_path = "sample_docs/ê¸°ì—… ë¬¸ì„œ ê²€ìƒ‰ ë„êµ¬ ë¶„ì„.md"
    
    if not os.path.exists(doc_path):
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {doc_path}")
        return
    
    with open(doc_path, 'r', encoding='utf-8') as f:
        content = f.read()
    
    print(f"\nğŸ“„ ì „ì²´ ë¬¸ì„œ ë¡œë“œ:")
    print(f"   ğŸ“ ì´ ë¬¸ì ìˆ˜: {len(content):,}")
    print(f"   ğŸ“ ì´ ì¤„ ìˆ˜: {content.count(chr(10)) + 1:,}")
    print(f"   ğŸ“Š ë‹¨ì–´ ìˆ˜ (ì¶”ì •): {len(content.split()):,}")
    
    headers = {
        "Authorization": f"Bearer {API_KEY}",
        "Content-Type": "application/json"
    }
    
    # 2. .env ì„¤ì •ìœ¼ë¡œ ì „ì²´ ë¬¸ì„œ ì²­í‚¹
    print(f"\nğŸ”ª ì „ì²´ ë¬¸ì„œ ì²­í‚¹ (.env ì„¤ì • ì‚¬ìš©)...")
    
    chunk_payload = {
        "text": content,
        "strategy": DEFAULT_CHUNK_STRATEGY,
        "chunk_size": DEFAULT_CHUNK_SIZE,
        "overlap": DEFAULT_CHUNK_OVERLAP,
        "language": DEFAULT_CHUNK_LANGUAGE
    }
    
    start_time = time.time()
    response = requests.post(f"{BASE_URL}/v1/chunk", headers=headers, json=chunk_payload)
    chunk_time = time.time() - start_time
    
    if response.status_code == 200:
        chunk_result = response.json()
        chunks = chunk_result["data"]
        
        print(f"âœ… ì²­í‚¹ ì„±ê³µ!")
        print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {chunk_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ì´ ì²­í¬ ìˆ˜: {len(chunks):,}")
        print(f"   ğŸ”¢ ì´ í† í° ìˆ˜: {chunk_result['total_tokens']:,}")
        print(f"   ğŸ“ í‰ê·  ì²­í¬ í¬ê¸°: {chunk_result['total_tokens'] / len(chunks):.1f} í† í°")
        print(f"   ğŸ“ˆ ì••ì¶•ë¥ : {chunk_result['original_length'] / chunk_result['total_tokens']:.2f} ë¬¸ì/í† í°")
        
        # ì²­í¬ í¬ê¸° ë¶„í¬
        token_counts = [chunk['token_count'] for chunk in chunks]
        min_tokens = min(token_counts)
        max_tokens = max(token_counts)
        print(f"   ğŸ“‰ í† í° ë²”ìœ„: {min_tokens} ~ {max_tokens}")
        
        # ì²˜ìŒ 3ê°œ ì²­í¬ ë¯¸ë¦¬ë³´ê¸°
        print(f"\nğŸ“ ì²­í¬ ë¯¸ë¦¬ë³´ê¸° (ì²˜ìŒ 3ê°œ):")
        for i, chunk in enumerate(chunks[:3]):
            preview = chunk['text'][:100].replace('\n', ' ') + "..."
            print(f"   ì²­í¬ {i+1}: {chunk['token_count']} í† í°")
            print(f"      {preview}")
        
        # 3. ì „ì²´ ì²­í¬ ì„ë² ë”© (ë°°ì¹˜ ì²˜ë¦¬)
        print(f"\nğŸ§  ì „ì²´ ì²­í¬ ì„ë² ë”© ìƒì„±...")
        
        batch_size = 5  # ì‘ì€ ë°°ì¹˜ë¡œ ì‹œì‘
        total_batches = (len(chunks) + batch_size - 1) // batch_size
        
        print(f"   ğŸ“Š ì„ë² ë”© ì²˜ë¦¬ ì •ë³´:")
        print(f"      ì´ ì²­í¬ ìˆ˜: {len(chunks):,}")
        print(f"      ë°°ì¹˜ í¬ê¸°: {batch_size}")
        print(f"      ì´ ë°°ì¹˜ ìˆ˜: {total_batches:,}")
        
        all_embeddings = []
        total_tokens_used = 0
        total_embed_time = 0
        
        for batch_idx in range(total_batches):
            start_idx = batch_idx * batch_size
            end_idx = min(start_idx + batch_size, len(chunks))
            batch_chunks = chunks[start_idx:end_idx]
            batch_texts = [chunk['text'] for chunk in batch_chunks]
            
            print(f"\n   ğŸ”„ ë°°ì¹˜ {batch_idx + 1}/{total_batches} ì²˜ë¦¬ ì¤‘... ({len(batch_texts)}ê°œ ì²­í¬)")
            
            embed_payload = {
                "input": batch_texts,
                "model": "nlpai-lab/KURE-v1"
            }
            
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/embeddings", headers=headers, json=embed_payload)
            batch_time = time.time() - start_time
            total_embed_time += batch_time
            
            if response.status_code == 200:
                embed_result = response.json()
                batch_embeddings = [data['embedding'] for data in embed_result['data']]
                all_embeddings.extend(batch_embeddings)
                
                batch_tokens = embed_result['usage']['total_tokens']
                total_tokens_used += batch_tokens
                
                print(f"      âœ… ì„±ê³µ: {len(batch_embeddings)}ê°œ ì„ë² ë”©, {batch_tokens} í† í°, {batch_time:.2f}ì´ˆ")
            else:
                print(f"      âŒ ì‹¤íŒ¨: {response.status_code} - {response.text}")
                break
        
        if all_embeddings:
            embedding_dim = len(all_embeddings[0])
            tokens_per_second = total_tokens_used / total_embed_time if total_embed_time > 0 else 0
            
            print(f"\nğŸ“ˆ ì„ë² ë”© ìƒì„± ì™„ë£Œ!")
            print(f"   âœ… ì´ ì„ë² ë”© ìˆ˜: {len(all_embeddings):,}")
            print(f"   ğŸ“ ì„ë² ë”© ì°¨ì›: {embedding_dim:,}")
            print(f"   â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_embed_time:.2f}ì´ˆ")
            print(f"   ğŸ”¢ ì´ í† í° ì‚¬ìš©ëŸ‰: {total_tokens_used:,}")
            print(f"   âš¡ ì²˜ë¦¬ ì†ë„: {tokens_per_second:.1f} í† í°/ì´ˆ")
            
            # 4. ìœ ì‚¬ë„ ë¶„ì„ (ì²˜ìŒ 5ê°œ ì²­í¬)
            print(f"\nğŸ”„ ìœ ì‚¬ë„ ë¶„ì„ (ì²˜ìŒ 5ê°œ ì²­í¬)...")
            
            sample_texts = [chunk['text'] for chunk in chunks[:5]]
            
            sim_payload = {
                "texts": sample_texts,
                "model": "nlpai-lab/KURE-v1"
            }
            
            start_time = time.time()
            response = requests.post(f"{BASE_URL}/similarity", headers=headers, json=sim_payload)
            sim_time = time.time() - start_time
            
            if response.status_code == 200:
                sim_result = response.json()
                similarities = sim_result['similarities']
                
                print(f"âœ… ìœ ì‚¬ë„ ê³„ì‚° ì„±ê³µ!")
                print(f"   â±ï¸  ì²˜ë¦¬ ì‹œê°„: {sim_time:.2f}ì´ˆ")
                print(f"   ğŸ“Š í–‰ë ¬ í¬ê¸°: {len(similarities)}x{len(similarities[0])}")
                
                # ìœ ì‚¬ë„ í–‰ë ¬ ì¶œë ¥
                print(f"\nğŸ“ˆ ìœ ì‚¬ë„ í–‰ë ¬:")
                print("     ", end="")
                for i in range(len(similarities)):
                    print(f"ì²­í¬{i+1:2d}", end="  ")
                print()
                
                for i, row in enumerate(similarities):
                    print(f"ì²­í¬{i+1:2d}", end="  ")
                    for val in row:
                        print(f"{val:.3f}", end="  ")
                    print()
                
                # ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ
                max_similarity = 0
                max_pair = (0, 0)
                for i in range(len(similarities)):
                    for j in range(i+1, len(similarities[i])):
                        if similarities[i][j] > max_similarity:
                            max_similarity = similarities[i][j]
                            max_pair = (i, j)
                
                print(f"\nğŸ† ê°€ì¥ ìœ ì‚¬í•œ ì²­í¬ ìŒ:")
                print(f"   ì²­í¬ {max_pair[0]+1} â†” ì²­í¬ {max_pair[1]+1} (ìœ ì‚¬ë„: {max_similarity:.4f})")
            else:
                print(f"âŒ ìœ ì‚¬ë„ ê³„ì‚° ì‹¤íŒ¨: {response.status_code}")
        
        # 5. ì „ì²´ ì²˜ë¦¬ ìš”ì•½
        total_time = chunk_time + total_embed_time
        print(f"\nğŸ“Š ì „ì²´ ì²˜ë¦¬ ìš”ì•½:")
        print(f"   ğŸ“„ ë¬¸ì„œ í¬ê¸°: {len(content):,} ë¬¸ì")
        print(f"   ğŸ”ª ì²­í‚¹ ì‹œê°„: {chunk_time:.2f}ì´ˆ")
        print(f"   ğŸ§  ì„ë² ë”© ì‹œê°„: {total_embed_time:.2f}ì´ˆ")
        print(f"   â±ï¸  ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
        print(f"   ğŸ“Š ì²­í¬ ìˆ˜: {len(chunks):,}")
        print(f"   ğŸ”¢ í† í° ì‚¬ìš©ëŸ‰: {total_tokens_used:,}")
        print(f"   âš¡ ì „ì²´ ì²˜ë¦¬ ì†ë„: {len(content) / total_time:.1f} ë¬¸ì/ì´ˆ")
        
        print(f"\nğŸ‰ .env ì„¤ì • ê¸°ë°˜ ì „ì²´ ë¬¸ì„œ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    else:
        print(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {response.status_code}")
        print(f"   ì˜¤ë¥˜: {response.text}")

if __name__ == "__main__":
    main()
