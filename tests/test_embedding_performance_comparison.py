#!/usr/bin/env python3
"""
ì„ë² ë”© ì„±ëŠ¥ ë° ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸
KURE ëª¨ë¸ê³¼ OpenAI ì„ë² ë”© ëª¨ë¸ì˜ ì†ë„ì™€ ì •í™•ë„ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.
"""

import os
import time
import json
import requests
import numpy as np
from typing import List, Dict, Any, Tuple
from datetime import datetime
from dotenv import load_dotenv
from openai import OpenAI
import statistics
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA


# Load environment variables
load_dotenv()


class EmbeddingPerformanceComparator:
    """ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ í´ë˜ìŠ¤"""
    
    def __init__(self):
        # API ì„¤ì •
        self.kure_api_key = os.getenv("API_KEY", "sk-kure-v1-test-key-12345")
        self.kure_base_url = os.getenv("BASE_URL", "http://localhost:8000")
        self.openai_api_key = os.getenv("OPENAI_API_KEY")
        
        # KURE API í—¤ë”
        self.kure_headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.kure_api_key}"
        }
        
        # OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        if self.openai_api_key:
            self.openai_client = OpenAI(api_key=self.openai_api_key)
        else:
            print("âš ï¸ OpenAI API key not found. OpenAI tests will be skipped.")
            self.openai_client = None
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.results = {}
        self.test_timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        
    def load_document(self, file_path: str) -> str:
        """ë¬¸ì„œ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {len(content)} ë¬¸ì")
            return content
        except Exception as e:
            print(f"âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return ""
    
    def chunk_document(self, text: str, chunk_size: int = 380, overlap: int = 70) -> List[str]:
        """KURE APIë¥¼ ì‚¬ìš©í•˜ì—¬ ë¬¸ì„œë¥¼ ì²­í‚¹í•©ë‹ˆë‹¤."""
        try:
            payload = {
                "text": text,
                "strategy": "recursive",
                "chunk_size": chunk_size,
                "overlap": overlap,
                "language": "ko"
            }
            
            response = requests.post(
                f"{self.kure_base_url}/v1/chunk",
                json=payload,
                headers=self.kure_headers
            )
            
            if response.status_code == 200:
                data = response.json()
                chunks = [chunk['text'] for chunk in data['data']]
                print(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")
                return chunks
            else:
                print(f"âŒ ì²­í‚¹ ì‹¤íŒ¨: {response.status_code}")
                return []
                
        except Exception as e:
            print(f"âŒ ì²­í‚¹ ì˜¤ë¥˜: {e}")
            return []

    def test_kure_embedding(self, chunks: List[str]) -> Dict[str, Any]:
        """KURE ì„ë² ë”© ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” KURE v1 ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        start_time = time.time()
        embeddings = []
        batch_times = []
        
        try:
            batch_size = 32  # KURE ìµœì  ë°°ì¹˜ í¬ê¸°
            
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                batch_start = time.time()
                
                payload = {
                    "input": batch,
                    "model": "nlpai-lab/KURE-v1"
                }
                
                response = requests.post(
                    f"{self.kure_base_url}/embeddings",
                    json=payload,
                    headers=self.kure_headers
                )
                
                batch_end = time.time()
                batch_time = batch_end - batch_start
                batch_times.append(batch_time)
                
                if response.status_code == 200:
                    data = response.json()
                    batch_embeddings = [item['embedding'] for item in data['data']]
                    embeddings.extend(batch_embeddings)
                    
                    print(f"   ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì²­í¬, {batch_time:.2f}ì´ˆ")
                else:
                    print(f"âŒ KURE ë°°ì¹˜ ì‹¤íŒ¨: {response.status_code}")
                    return {"success": False, "error": f"HTTP {response.status_code}"}
        
        except Exception as e:
            print(f"âŒ KURE ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}
        
        total_time = time.time() - start_time
        
        result = {
            "success": True,
            "model": "KURE-v1",
            "total_chunks": len(chunks),
            "total_time": total_time,
            "avg_time_per_chunk": total_time / len(chunks),
            "chunks_per_second": len(chunks) / total_time,
            "batch_times": batch_times,
            "avg_batch_time": statistics.mean(batch_times),
            "embeddings": embeddings,
            "embedding_dimension": len(embeddings[0]) if embeddings else 0
        }
        
        print(f"âœ… KURE í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {total_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {result['chunks_per_second']:.2f} ì²­í¬/ì´ˆ")
        
        return result

    def test_openai_embedding(self, chunks: List[str], model: str) -> Dict[str, Any]:
        """OpenAI ì„ë² ë”© ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        if not self.openai_client:
            return {"success": False, "error": "OpenAI client not available"}
        
        print(f"\nğŸ” OpenAI {model} ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹œì‘...")
        
        start_time = time.time()
        embeddings = []
        batch_times = []
        total_tokens = 0
        
        try:
            batch_size = 100  # OpenAI API ì œí•œ ê³ ë ¤
            
            for i in range(0, len(chunks), batch_size):
                batch = chunks[i:i + batch_size]
                batch_start = time.time()
                
                response = self.openai_client.embeddings.create(
                    input=batch,
                    model=model
                )
                
                batch_end = time.time()
                batch_time = batch_end - batch_start
                batch_times.append(batch_time)
                
                batch_embeddings = [data.embedding for data in response.data]
                embeddings.extend(batch_embeddings)
                total_tokens += response.usage.total_tokens
                
                print(f"   ë°°ì¹˜ {i//batch_size + 1}: {len(batch)}ê°œ ì²­í¬, {batch_time:.2f}ì´ˆ")
                
                # API ì œí•œ ì¤€ìˆ˜
                time.sleep(0.1)
        
        except Exception as e:
            print(f"âŒ OpenAI ì„ë² ë”© ì˜¤ë¥˜: {e}")
            return {"success": False, "error": str(e)}
        
        total_time = time.time() - start_time
        
        result = {
            "success": True,
            "model": model,
            "total_chunks": len(chunks),
            "total_time": total_time,
            "avg_time_per_chunk": total_time / len(chunks),
            "chunks_per_second": len(chunks) / total_time,
            "batch_times": batch_times,
            "avg_batch_time": statistics.mean(batch_times),
            "embeddings": embeddings,
            "embedding_dimension": len(embeddings[0]) if embeddings else 0,
            "total_tokens": total_tokens
        }
        
        print(f"âœ… OpenAI {model} í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬, {total_time:.2f}ì´ˆ")
        print(f"   í‰ê·  ì†ë„: {result['chunks_per_second']:.2f} ì²­í¬/ì´ˆ")
        print(f"   ì´ í† í°: {total_tokens}")
        
        return result

    def analyze_embedding_quality(self, embeddings: List[List[float]], model_name: str) -> Dict[str, Any]:
        """ì„ë² ë”© í’ˆì§ˆì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“Š {model_name} ì„ë² ë”© í’ˆì§ˆ ë¶„ì„...")

        embeddings_array = np.array(embeddings)

        # 1. ê¸°ë³¸ í†µê³„
        norms = np.linalg.norm(embeddings_array, axis=1)

        # 2. ìœ ì‚¬ë„ ë¶„í¬ ë¶„ì„
        similarity_matrix = cosine_similarity(embeddings_array)
        # ëŒ€ê°ì„  ì œì™¸ (ìê¸° ìì‹ ê³¼ì˜ ìœ ì‚¬ë„)
        upper_triangle = np.triu(similarity_matrix, k=1)
        similarities = upper_triangle[upper_triangle != 0]

        # 3. ì°¨ì›ë³„ ë¶„ì‚° ë¶„ì„
        dimension_variances = np.var(embeddings_array, axis=0)

        # 4. í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ (ìƒ˜í”Œì´ ì¶©ë¶„í•œ ê²½ìš°)
        clustering_score = None
        if len(embeddings) >= 10:
            try:
                n_clusters = min(5, len(embeddings) // 2)
                kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
                cluster_labels = kmeans.fit_predict(embeddings_array)

                # ì‹¤ë£¨ì—£ ìŠ¤ì½”ì–´ ê³„ì‚°
                from sklearn.metrics import silhouette_score
                clustering_score = silhouette_score(embeddings_array, cluster_labels)
            except Exception as e:
                print(f"   í´ëŸ¬ìŠ¤í„°ë§ ë¶„ì„ ì‹¤íŒ¨: {e}")

        quality_metrics = {
            "embedding_count": len(embeddings),
            "dimension": len(embeddings[0]),
            "norm_stats": {
                "mean": float(np.mean(norms)),
                "std": float(np.std(norms)),
                "min": float(np.min(norms)),
                "max": float(np.max(norms))
            },
            "similarity_stats": {
                "mean": float(np.mean(similarities)),
                "std": float(np.std(similarities)),
                "min": float(np.min(similarities)),
                "max": float(np.max(similarities)),
                "median": float(np.median(similarities))
            },
            "dimension_variance_stats": {
                "mean": float(np.mean(dimension_variances)),
                "std": float(np.std(dimension_variances)),
                "min": float(np.min(dimension_variances)),
                "max": float(np.max(dimension_variances))
            },
            "clustering_score": clustering_score
        }

        print(f"   ì„ë² ë”© ê°œìˆ˜: {quality_metrics['embedding_count']}")
        print(f"   ì°¨ì›: {quality_metrics['dimension']}")
        print(f"   í‰ê·  ë…¸ë¦„: {quality_metrics['norm_stats']['mean']:.4f}")
        print(f"   í‰ê·  ìœ ì‚¬ë„: {quality_metrics['similarity_stats']['mean']:.4f}")
        if clustering_score:
            print(f"   í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜: {clustering_score:.4f}")

        return quality_metrics

    def compare_models(self, chunks: List[str]) -> Dict[str, Any]:
        """ëª¨ë“  ëª¨ë¸ì„ ë¹„êµí•©ë‹ˆë‹¤."""
        print(f"\nğŸš€ ì„ë² ë”© ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ ì‹œì‘")
        print(f"ğŸ“„ ì´ {len(chunks)}ê°œ ì²­í¬ë¡œ í…ŒìŠ¤íŠ¸")
        print("=" * 80)

        results = {}

        # KURE v1 í…ŒìŠ¤íŠ¸
        kure_result = self.test_kure_embedding(chunks)
        if kure_result["success"]:
            kure_result["quality"] = self.analyze_embedding_quality(
                kure_result["embeddings"], "KURE v1"
            )
            results["kure_v1"] = kure_result

        # OpenAI text-embedding-3-small í…ŒìŠ¤íŠ¸
        if self.openai_client:
            openai_small_result = self.test_openai_embedding(chunks, "text-embedding-3-small")
            if openai_small_result["success"]:
                openai_small_result["quality"] = self.analyze_embedding_quality(
                    openai_small_result["embeddings"], "OpenAI text-embedding-3-small"
                )
                results["openai_small"] = openai_small_result

            # OpenAI text-embedding-3-large í…ŒìŠ¤íŠ¸
            openai_large_result = self.test_openai_embedding(chunks, "text-embedding-3-large")
            if openai_large_result["success"]:
                openai_large_result["quality"] = self.analyze_embedding_quality(
                    openai_large_result["embeddings"], "OpenAI text-embedding-3-large"
                )
                results["openai_large"] = openai_large_result

        return results

    def generate_performance_report(self, results: Dict[str, Any], chunks: List[str]) -> Dict[str, Any]:
        """ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“‹ ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±...")

        report = {
            "test_info": {
                "timestamp": self.test_timestamp,
                "total_chunks": len(chunks),
                "chunk_sample": chunks[0][:100] + "..." if chunks else "",
                "models_tested": list(results.keys())
            },
            "performance_comparison": {},
            "quality_comparison": {},
            "summary": {}
        }

        # ì„±ëŠ¥ ë¹„êµ
        for model_name, result in results.items():
            if result["success"]:
                report["performance_comparison"][model_name] = {
                    "total_time": result["total_time"],
                    "avg_time_per_chunk": result["avg_time_per_chunk"],
                    "chunks_per_second": result["chunks_per_second"],
                    "embedding_dimension": result["embedding_dimension"]
                }

                if "total_tokens" in result:
                    report["performance_comparison"][model_name]["total_tokens"] = result["total_tokens"]

                # í’ˆì§ˆ ë¹„êµ
                if "quality" in result:
                    report["quality_comparison"][model_name] = result["quality"]

        # ìš”ì•½ ìƒì„±
        if results:
            fastest_model = min(results.keys(),
                              key=lambda x: results[x]["total_time"] if results[x]["success"] else float('inf'))

            report["summary"] = {
                "fastest_model": fastest_model,
                "speed_comparison": {},
                "quality_insights": {}
            }

            # ì†ë„ ë¹„êµ
            fastest_time = results[fastest_model]["total_time"]
            for model_name, result in results.items():
                if result["success"]:
                    speed_ratio = result["total_time"] / fastest_time
                    report["summary"]["speed_comparison"][model_name] = {
                        "relative_speed": speed_ratio,
                        "speed_description": f"{speed_ratio:.1f}x slower" if speed_ratio > 1 else "fastest"
                    }

            # í’ˆì§ˆ ì¸ì‚¬ì´íŠ¸
            if all("quality" in results[k] for k in results.keys() if results[k]["success"]):
                quality_scores = {}
                for model_name, result in results.items():
                    if result["success"] and "quality" in result:
                        quality_scores[model_name] = result["quality"]["similarity_stats"]["mean"]

                best_quality_model = max(quality_scores.keys(), key=lambda x: quality_scores[x])
                report["summary"]["quality_insights"] = {
                    "best_quality_model": best_quality_model,
                    "quality_scores": quality_scores
                }

        return report

    def save_results(self, report: Dict[str, Any]) -> str:
        """ê²°ê³¼ë¥¼ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        output_dir = f"test_outputs/{self.test_timestamp}"
        os.makedirs(output_dir, exist_ok=True)

        # JSON ë¦¬í¬íŠ¸ ì €ì¥
        json_file = f"{output_dir}/embedding_performance_comparison.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_file = f"{output_dir}/performance_summary.md"
        self.generate_markdown_report(report, summary_file)

        print(f"âœ… ê²°ê³¼ ì €ì¥ ì™„ë£Œ:")
        print(f"   ğŸ“„ JSON ë¦¬í¬íŠ¸: {json_file}")
        print(f"   ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")

        return output_dir

    def generate_markdown_report(self, report: Dict[str, Any], file_path: str):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ì˜ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write(f"# ì„ë² ë”© ì„±ëŠ¥ ë¹„êµ ë¦¬í¬íŠ¸\n\n")
            f.write(f"**í…ŒìŠ¤íŠ¸ ì‹œê°„**: {report['test_info']['timestamp']}\n")
            f.write(f"**ì´ ì²­í¬ ìˆ˜**: {report['test_info']['total_chunks']}\n")
            f.write(f"**í…ŒìŠ¤íŠ¸ ëª¨ë¸**: {', '.join(report['test_info']['models_tested'])}\n\n")

            # ì„±ëŠ¥ ë¹„êµ
            f.write("## ğŸš€ ì„±ëŠ¥ ë¹„êµ\n\n")
            f.write("| ëª¨ë¸ | ì´ ì‹œê°„(ì´ˆ) | ì²­í¬/ì´ˆ | ì°¨ì› | í† í° ìˆ˜ |\n")
            f.write("|------|-------------|---------|------|--------|\n")

            for model_name, perf in report["performance_comparison"].items():
                tokens = perf.get("total_tokens", "N/A")
                f.write(f"| {model_name} | {perf['total_time']:.2f} | {perf['chunks_per_second']:.2f} | {perf['embedding_dimension']} | {tokens} |\n")

            # í’ˆì§ˆ ë¹„êµ
            f.write("\n## ğŸ“Š í’ˆì§ˆ ë¹„êµ\n\n")
            f.write("| ëª¨ë¸ | í‰ê·  ìœ ì‚¬ë„ | ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨ | í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜ |\n")
            f.write("|------|-------------|----------------|----------------|\n")

            for model_name, quality in report["quality_comparison"].items():
                clustering = quality.get("clustering_score", "N/A")
                if clustering != "N/A" and clustering is not None:
                    clustering = f"{clustering:.4f}"
                f.write(f"| {model_name} | {quality['similarity_stats']['mean']:.4f} | {quality['similarity_stats']['std']:.4f} | {clustering} |\n")

            # ìš”ì•½
            if "summary" in report and report["summary"]:
                f.write("\n## ğŸ“‹ ìš”ì•½\n\n")
                f.write(f"**ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸**: {report['summary']['fastest_model']}\n\n")

                if "quality_insights" in report["summary"]:
                    f.write(f"**ê°€ì¥ ë†’ì€ í’ˆì§ˆ**: {report['summary']['quality_insights']['best_quality_model']}\n\n")

                f.write("### ì†ë„ ë¹„êµ\n")
                for model_name, speed_info in report["summary"]["speed_comparison"].items():
                    f.write(f"- **{model_name}**: {speed_info['speed_description']}\n")

    def run_full_test(self, document_path: str) -> str:
        """ì „ì²´ í…ŒìŠ¤íŠ¸ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸ§ª ì„ë² ë”© ì„±ëŠ¥ ë° ì •í™•ë„ ë¹„êµ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        print("=" * 80)

        # 1. ë¬¸ì„œ ë¡œë“œ
        document_content = self.load_document(document_path)
        if not document_content:
            print("âŒ ë¬¸ì„œ ë¡œë“œ ì‹¤íŒ¨")
            return ""

        # 2. ë¬¸ì„œ ì²­í‚¹
        chunks = self.chunk_document(document_content)
        if not chunks:
            print("âŒ ë¬¸ì„œ ì²­í‚¹ ì‹¤íŒ¨")
            return ""

        # 3. ëª¨ë¸ ë¹„êµ
        results = self.compare_models(chunks)
        if not results:
            print("âŒ ëª¨ë¸ ë¹„êµ ì‹¤íŒ¨")
            return ""

        # 4. ë¦¬í¬íŠ¸ ìƒì„±
        report = self.generate_performance_report(results, chunks)

        # 5. ê²°ê³¼ ì €ì¥
        output_dir = self.save_results(report)

        # 6. ê²°ê³¼ ì¶œë ¥
        self.print_summary(report)

        return output_dir

    def print_summary(self, report: Dict[str, Any]):
        """ê²°ê³¼ ìš”ì•½ì„ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"\nğŸ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("=" * 80)

        print(f"\nğŸ“Š ì„±ëŠ¥ ìš”ì•½:")
        for model_name, perf in report["performance_comparison"].items():
            print(f"  {model_name}:")
            print(f"    - ì´ ì‹œê°„: {perf['total_time']:.2f}ì´ˆ")
            print(f"    - ì²˜ë¦¬ ì†ë„: {perf['chunks_per_second']:.2f} ì²­í¬/ì´ˆ")
            print(f"    - ì„ë² ë”© ì°¨ì›: {perf['embedding_dimension']}")
            if "total_tokens" in perf:
                print(f"    - ì´ í† í°: {perf['total_tokens']}")

        print(f"\nğŸ“ˆ í’ˆì§ˆ ìš”ì•½:")
        for model_name, quality in report["quality_comparison"].items():
            print(f"  {model_name}:")
            print(f"    - í‰ê·  ìœ ì‚¬ë„: {quality['similarity_stats']['mean']:.4f}")
            print(f"    - ìœ ì‚¬ë„ í‘œì¤€í¸ì°¨: {quality['similarity_stats']['std']:.4f}")
            if quality.get("clustering_score"):
                print(f"    - í´ëŸ¬ìŠ¤í„°ë§ ì ìˆ˜: {quality['clustering_score']:.4f}")

        if "summary" in report and report["summary"]:
            print(f"\nğŸ† ìµœì¢… ê²°ê³¼:")
            print(f"  ê°€ì¥ ë¹ ë¥¸ ëª¨ë¸: {report['summary']['fastest_model']}")
            if "quality_insights" in report["summary"]:
                print(f"  ê°€ì¥ ë†’ì€ í’ˆì§ˆ: {report['summary']['quality_insights']['best_quality_model']}")


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    # ë¬¸ì„œ ê²½ë¡œ
    document_path = "sample_docs/P02_01_01_001_20210101_marker.md"

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    comparator = EmbeddingPerformanceComparator()
    output_dir = comparator.run_full_test(document_path)

    if output_dir:
        print(f"\nğŸ“ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_dir}")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")


if __name__ == "__main__":
    main()
