"""Document processing service for uploaded files."""

import time
import uuid
import logging
from pathlib import Path
from typing import Dict, List, Optional, Any
from models import SupportedFileType
from services.file_upload_service import file_upload_service
from services.marker_service import marker_service
from services.docling_service import docling_service
from services import chunking_service, embedding_service
from services.unified_search_service import unified_search_service
from config import settings

logger = logging.getLogger(__name__)


class DocumentProcessingService:
    """Service for processing uploaded documents."""

    def __init__(self):
        logger.info("Document processing service initialized")
    
    def _choose_conversion_method(self, file_type: SupportedFileType, method: str = "auto") -> str:
        """Choose the best conversion method for the file type."""
        if method in ["marker", "docling"]:
            return method

        # Auto selection logic - choose best engine for each format
        if file_type == SupportedFileType.PDF:
            return "marker"  # Marker excels at PDF conversion
        elif file_type in [SupportedFileType.DOCX, SupportedFileType.PPTX, SupportedFileType.XLSX]:
            return "docling"  # Docling has native Office document support
        else:
            return "docling"  # Fallback to docling for other formats
    
    def _read_text_file(self, file_path: Path) -> str:
        """Read content from text files."""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            # Try with different encoding
            with open(file_path, 'r', encoding='latin-1') as f:
                return f.read()
    
    async def _convert_document(self, file_path: Path, file_type: SupportedFileType,
                               method: str, extract_images: bool = False) -> Dict:
        """Convert document to markdown."""
        logger.info(f"ðŸ“„ ë¬¸ì„œ ë³€í™˜ ì‹œìž‘: {file_path.name} (íƒ€ìž…: {file_type.value}, ë°©ë²•: {method})")
        start_time = time.time()

        try:
            if file_type in [SupportedFileType.TXT, SupportedFileType.MD]:
                # For text files, just read the content
                logger.info(f"ðŸ“– í…ìŠ¤íŠ¸ íŒŒì¼ ì§ì ‘ ì½ê¸°")
                content = self._read_text_file(file_path)
                logger.info(f"âœ… í…ìŠ¤íŠ¸ ì½ê¸° ì™„ë£Œ: {len(content)} ë¬¸ìž")
                return {
                    "success": True,
                    "markdown_content": content,
                    "markdown_length": len(content),
                    "conversion_time": 0.1,
                    "method_used": "direct_read"
                }
            
            elif file_type == SupportedFileType.PDF:
                logger.info(f"ðŸ“„ PDF ë³€í™˜ ì‹œìž‘ - ë°©ë²•: {method}")
                if method == "marker":
                    logger.info(f"ðŸ”„ Marker ì„œë¹„ìŠ¤ë¡œ PDF ë³€í™˜ ì¤‘...")
                    result = marker_service.convert_pdf_to_markdown(
                        pdf_path=str(file_path),
                        output_dir="temp_processing",
                        extract_images=extract_images
                    )
                else:  # docling
                    logger.info(f"ðŸ”„ Docling ì„œë¹„ìŠ¤ë¡œ PDF ë³€í™˜ ì¤‘...")
                    result = docling_service.convert_pdf_to_markdown(
                        pdf_path=str(file_path),
                        output_dir="temp_processing",
                        extract_images=extract_images
                    )
                
                if result.get("success"):
                    conversion_time = time.time() - start_time
                    markdown_length = result.get("markdown_length", 0)
                    logger.info(f"âœ… PDF ë³€í™˜ ì„±ê³µ: {markdown_length} ë¬¸ìž, {conversion_time:.2f}ì´ˆ")
                    return {
                        "success": True,
                        "markdown_content": result.get("markdown", ""),
                        "markdown_length": markdown_length,
                        "conversion_time": conversion_time,
                        "method_used": method
                    }
                else:
                    logger.error(f"âŒ PDF ë³€í™˜ ì‹¤íŒ¨: {result.get('error', 'Conversion failed')}")
                    return {
                        "success": False,
                        "error": result.get("error", "Conversion failed"),
                        "method_used": method
                    }
            
            elif file_type in [SupportedFileType.DOCX, SupportedFileType.PPTX, SupportedFileType.XLSX]:
                # Office documents - use Docling (recommended) or fallback to Marker
                logger.info(f"ðŸ“„ Office ë¬¸ì„œ ë³€í™˜ ì‹œìž‘ - ë°©ë²•: {method}")
                if method == "docling" or method == "auto":
                    logger.info(f"ðŸ”„ Docling ì„œë¹„ìŠ¤ë¡œ Office ë¬¸ì„œ ë³€í™˜ ì¤‘...")
                    result = docling_service.convert_office_to_markdown(
                        file_path=str(file_path),
                        output_dir="temp_processing"
                    )
                else:  # marker (fallback, but not ideal for Office docs)
                    logger.warning(f"âš ï¸ MarkerëŠ” Office ë¬¸ì„œì— ìµœì í™”ë˜ì§€ ì•ŠìŒ. Docling ì‚¬ìš©ì„ ê¶Œìž¥í•©ë‹ˆë‹¤.")
                    logger.info(f"ðŸ”„ Marker ì„œë¹„ìŠ¤ë¡œ Office ë¬¸ì„œ ë³€í™˜ ì‹œë„ ì¤‘...")
                    # Marker doesn't directly support Office docs, so this might fail
                    try:
                        result = marker_service.convert_pdf_to_markdown(
                            pdf_path=str(file_path),
                            output_dir="temp_processing",
                            extract_images=extract_images
                        )
                    except Exception as e:
                        logger.error(f"Markerë¡œ Office ë¬¸ì„œ ë³€í™˜ ì‹¤íŒ¨: {e}")
                        logger.info("Doclingìœ¼ë¡œ ëŒ€ì²´ ì‹œë„...")
                        result = docling_service.convert_office_to_markdown(
                            file_path=str(file_path),
                            output_dir="temp_processing"
                        )

                if result.get("success"):
                    return {
                        "success": True,
                        "markdown_content": result.get("markdown", ""),
                        "markdown_length": result.get("markdown_length", 0),
                        "conversion_time": result.get("conversion_time", 0),
                        "method_used": method
                    }
                else:
                    return {
                        "success": False,
                        "error": result.get("error", "Conversion failed"),
                        "method_used": method
                    }
            
            else:
                return {
                    "success": False,
                    "error": f"Unsupported file type for conversion: {file_type}",
                    "method_used": method
                }
                
        except Exception as e:
            logger.error(f"Error converting document: {str(e)}")
            return {
                "success": False,
                "error": f"Conversion error: {str(e)}",
                "method_used": method
            }
    
    def _chunk_text(self, text: str, strategy: str, chunk_size: int, overlap: int) -> List[Dict]:
        """Chunk text into smaller pieces."""
        try:
            chunks = chunking_service.chunk_text(
                text=text,
                strategy=strategy,
                chunk_size=chunk_size,
                overlap=overlap,
                language="auto"
            )
            
            return [
                {
                    "text": chunk.text,
                    "index": i,
                    "start_char": chunk.start_char,
                    "end_char": chunk.end_char,
                    "token_count": chunk.token_count
                }
                for i, chunk in enumerate(chunks)
            ]
            
        except Exception as e:
            logger.error(f"Error chunking text: {str(e)}")
            return []
    
    def _generate_embeddings(self, chunks: List[Dict], model: str) -> bool:
        """Generate embeddings for text chunks."""
        try:
            logger.info(f"ðŸ”¢ ìž„ë² ë”© ìƒì„± ì‹œìž‘: {len(chunks)}ê°œ ì²­í¬, ëª¨ë¸={model}")
            texts = [chunk["text"] for chunk in chunks]
            logger.info(f"ðŸ“ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì™„ë£Œ: í‰ê·  ê¸¸ì´={sum(len(t) for t in texts) / len(texts):.1f}ìž")

            embeddings = embedding_service.encode_texts(texts, model)
            logger.info(f"ðŸ§  ìž„ë² ë”© ì¸ì½”ë”© ì™„ë£Œ: shape={embeddings.shape}")

            # Add embeddings to chunks
            for i, chunk in enumerate(chunks):
                chunk["embedding"] = embeddings[i].tolist()

            logger.info(f"âœ… ìž„ë² ë”© ìƒì„± ì„±ê³µ: {len(chunks)}ê°œ ì²­í¬ ì²˜ë¦¬ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"âŒ ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨: {str(e)}")
            import traceback
            logger.error(f"ìƒì„¸ ì˜¤ë¥˜: {traceback.format_exc()}")
            return False
    
    async def process_document(self, file_id: str, conversion_method: str = "auto",
                              extract_images: bool = False, chunk_strategy: Optional[str] = None,
                              chunk_size: Optional[int] = None, overlap: Optional[int] = None,
                              generate_embeddings: bool = True, embedding_model: Optional[str] = None,
                              enable_hash_check: Optional[bool] = None) -> Dict:
        """Process uploaded document through the full pipeline."""
        start_time = time.time()
        
        try:
            # Get file info
            file_info = file_upload_service.get_file_info(file_id)
            if not file_info:
                return {
                    "success": False,
                    "error": f"File not found: {file_id}",
                    "processing_time": time.time() - start_time
                }

            # Check if this is a duplicate file and if it's already been processed
            from services.database_service import database_service
            file_hash = file_info.get("file_hash")

            # Determine if hash check should be enabled for this request
            # Priority: request parameter > system default setting
            hash_check_enabled = enable_hash_check if enable_hash_check is not None else settings.enable_hash_duplicate_check

            # Check for duplicate documents using hash - only if enabled
            existing_document = None
            if file_hash and hash_check_enabled:
                logger.info(f"ðŸ” ì¤‘ë³µ ë¬¸ì„œ ê²€ì‚¬ ì‹œìž‘ (ìš”ì²­ë³„ ì„¤ì •: {enable_hash_check}, ì‹œìŠ¤í…œ ê¸°ë³¸ê°’: {settings.enable_hash_duplicate_check}): {file_hash[:16]}...")
                existing_document = database_service.find_document_by_file_hash(file_hash)

                if existing_document:
                    logger.info(f"ðŸ“‹ ê¸°ì¡´ ì²˜ë¦¬ëœ ë¬¸ì„œ ë°œê²¬: {existing_document['filename']} (ë¬¸ì„œ ID: {existing_document['id']})")

                    # Return existing document information
                    return {
                        "success": True,
                        "duplicate_detected": True,
                        "existing_document": True,
                        "file_id": file_id,
                        "document_id": existing_document["id"],
                        "filename": file_info["filename"],
                        "original_filename": existing_document["filename"],
                        "conversion_method": existing_document["conversion_method"],
                        "conversion_time": existing_document["conversion_time"],
                        "markdown_content": existing_document["markdown_content"],
                        "markdown_length": existing_document.get("markdown_length", 0),
                        "total_chunks": existing_document.get("total_chunks", 0),
                        "chunks": existing_document.get("chunks", []),
                        "embeddings_generated": existing_document.get("embeddings_generated", False),
                        "processing_time": time.time() - start_time,
                        "original_processing_time": existing_document.get("processing_time", 0),
                        "original_created_at": existing_document.get("created_at", 0),
                        "message": f"ë™ì¼í•œ íŒŒì¼ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤: {existing_document['filename']}"
                    }
                else:
                    logger.info(f"âœ… ìƒˆë¡œìš´ íŒŒì¼ - ë¬¸ì„œ ì²˜ë¦¬ ì§„í–‰")
            elif file_hash and not hash_check_enabled:
                logger.info(f"ðŸ” ì¤‘ë³µ ë¬¸ì„œ ê²€ì‚¬ ë¹„í™œì„±í™”ë¨ (ìš”ì²­ ì„¤ì •: {enable_hash_check}, ì‹œìŠ¤í…œ ê¸°ë³¸ê°’: {settings.enable_hash_duplicate_check})")
            else:
                logger.warning(f"âš ï¸ íŒŒì¼ í•´ì‹œ ì •ë³´ ì—†ìŒ - ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ")
            
            file_path = Path(file_info["temp_path"])
            if not file_path.exists():
                return {
                    "success": False,
                    "error": f"File not found on disk: {file_path}",
                    "processing_time": time.time() - start_time
                }
            
            # Set defaults
            chunk_strategy = chunk_strategy or settings.default_chunk_strategy
            chunk_size = chunk_size or settings.default_chunk_size
            overlap = overlap or settings.default_chunk_overlap
            embedding_model = embedding_model or settings.default_model
            
            # Choose conversion method
            file_type_str = file_info["file_type"]
            # Convert string to SupportedFileType enum
            try:
                file_type = SupportedFileType(file_type_str.lower())
            except ValueError:
                # Fallback for common extensions
                if file_type_str.lower() == "pdf":
                    file_type = SupportedFileType.PDF
                elif file_type_str.lower() in ["txt", "text"]:
                    file_type = SupportedFileType.TXT
                elif file_type_str.lower() in ["md", "markdown"]:
                    file_type = SupportedFileType.MD
                else:
                    file_type = SupportedFileType.PDF  # Default fallback

            logger.info(f"ðŸ“‹ íŒŒì¼ íƒ€ìž… í™•ì¸: {file_type_str} -> {file_type}")
            method = self._choose_conversion_method(file_type, conversion_method)
            
            # Convert document
            conversion_result = await self._convert_document(
                file_path, file_type, method, extract_images
            )
            
            if not conversion_result.get("success"):
                return {
                    "success": False,
                    "error": conversion_result.get("error", "Conversion failed"),
                    "processing_time": time.time() - start_time
                }
            
            markdown_content = conversion_result["markdown_content"]
            
            # Chunk text
            logger.info(f"âœ‚ï¸ í…ìŠ¤íŠ¸ ì²­í‚¹ ì‹œìž‘: ì „ëžµ={chunk_strategy}, í¬ê¸°={chunk_size}, ì˜¤ë²„ëž©={overlap}")
            chunks = self._chunk_text(markdown_content, chunk_strategy, chunk_size, overlap)
            logger.info(f"âœ… ì²­í‚¹ ì™„ë£Œ: {len(chunks)}ê°œ ì²­í¬ ìƒì„±")

            # Generate embeddings if requested
            embeddings_generated = False
            if generate_embeddings and chunks:
                logger.info(f"ðŸ”¢ ìž„ë² ë”© ìƒì„± ì‹œìž‘: ëª¨ë¸={embedding_model}")
                embeddings_generated = self._generate_embeddings(chunks, embedding_model)
                if embeddings_generated:
                    logger.info(f"âœ… ìž„ë² ë”© ìƒì„± ì™„ë£Œ")
                else:
                    logger.warning(f"âš ï¸ ìž„ë² ë”© ìƒì„± ì‹¤íŒ¨")
            
            # Generate document ID
            document_id = str(uuid.uuid4())

            # Store processed content using storage service
            from services.storage_service import storage_service

            # Store markdown content
            markdown_storage = storage_service.store_processed_content(
                file_id=file_id,
                content_type="markdown",
                content=markdown_content,
                filename=f"{document_id}_converted.md"
            )

            # Store chunks as JSON
            import json
            chunks_json = json.dumps(chunks, ensure_ascii=False, indent=2)
            chunks_storage = storage_service.store_processed_content(
                file_id=file_id,
                content_type="chunks",
                content=chunks_json,
                filename=f"{document_id}_chunks.json"
            )

            # Store processed document metadata
            processed_doc = {
                "document_id": document_id,
                "file_id": file_id,
                "filename": file_info["filename"],
                "file_type": file_type,
                "conversion_method": method,
                "conversion_time": conversion_result["conversion_time"],
                "markdown_content": markdown_content,
                "markdown_storage_path": markdown_storage["storage_path"],
                "chunks": chunks,
                "chunks_storage_path": chunks_storage["storage_path"],
                "embeddings_generated": embeddings_generated,
                "processing_time": time.time() - start_time,
                "created_at": time.time()
            }

            # Store in database with error handling
            from services.database_service import database_service
            try:
                success = database_service.store_document(processed_doc)
                if not success:
                    logger.error(f"ë¬¸ì„œ ë°ì´í„°ë² ì´ìŠ¤ ì €ìž¥ ì‹¤íŒ¨: {document_id}")
                    # Clean up stored files
                    try:
                        Path(markdown_storage["storage_path"]).unlink(missing_ok=True)
                        Path(chunks_storage["storage_path"]).unlink(missing_ok=True)
                    except Exception as cleanup_error:
                        logger.error(f"ì²˜ë¦¬ëœ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")

                    return {
                        "success": False,
                        "error": "Failed to store document in database",
                        "processing_time": time.time() - start_time
                    }
            except Exception as db_error:
                logger.error(f"ë¬¸ì„œ ì €ìž¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {db_error}")
                # Clean up stored files
                try:
                    Path(markdown_storage["storage_path"]).unlink(missing_ok=True)
                    Path(chunks_storage["storage_path"]).unlink(missing_ok=True)
                except Exception as cleanup_error:
                    logger.error(f"ì²˜ë¦¬ëœ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {cleanup_error}")

                return {
                    "success": False,
                    "error": f"Database error during document storage: {str(db_error)}",
                    "processing_time": time.time() - start_time
                }

            # Store in unified search service if embeddings were generated
            if embeddings_generated and chunks:
                try:

                    # Check if document with same hash already exists in vector DB - only if enabled
                    existing_in_vector_db = None
                    if file_hash and hash_check_enabled:
                        logger.info(f"ðŸ” ë²¡í„° DB ì¤‘ë³µ ê²€ì‚¬ ì‹œìž‘")
                        existing_in_vector_db = await unified_search_service.check_document_exists_by_hash(file_hash)
                    elif file_hash and not hash_check_enabled:
                        logger.info(f"ðŸ” ë²¡í„° DB ì¤‘ë³µ ê²€ì‚¬ ë¹„í™œì„±í™”ë¨ (ìš”ì²­ ì„¤ì •: {enable_hash_check}, ì‹œìŠ¤í…œ ê¸°ë³¸ê°’: {settings.enable_hash_duplicate_check})")

                        if existing_in_vector_db:
                            logger.info(f"ðŸ“‹ ë²¡í„° DBì— ë™ì¼ ë¬¸ì„œ ì¡´ìž¬ - ì €ìž¥ ìŠ¤í‚µ: {existing_in_vector_db}")
                        else:
                            logger.info(f"âœ… ë²¡í„° DBì— ìƒˆ ë¬¸ì„œ ì €ìž¥ ì§„í–‰")

                            # Prepare documents for unified search service
                            # 1. Qdrantìš© ì²­í¬ ë¬¸ì„œë“¤ (ë²¡í„° ê²€ìƒ‰ìš©)
                            chunk_documents = []
                            for i, chunk in enumerate(chunks):
                                # ì²­í‚¹ ì„œë¹„ìŠ¤ì—ì„œ text í•„ë“œì— í…ìŠ¤íŠ¸ë¥¼ ì €ìž¥í•˜ë¯€ë¡œ ì´ë¥¼ contentë¡œ ë§¤í•‘
                                chunk_text = chunk.get("text", "")
                                doc = {
                                    "id": f"{document_id}_chunk_{i}",
                                    "document_id": document_id,
                                    "embedding": chunk.get("embedding"),
                                    "content": chunk_text,  # text í•„ë“œë¥¼ contentë¡œ ë§¤í•‘
                                    "title": file_info["filename"],
                                    "file_name": file_info["filename"],
                                    "file_type": file_type.value,
                                    "chunk_index": i,
                                    "file_size": file_info.get("size", 0),
                                    "created_at": time.time(),
                                    "metadata": {
                                        "filename": file_info["filename"],
                                        "file_type": file_type.value,
                                        "conversion_method": method,
                                        "created_at": time.time(),
                                        "embedding_model": embedding_model,
                                        "document_id": document_id,
                                        "chunk_index": i,
                                        "text": chunk_text,  # Qdrantìš© text í•„ë“œ
                                        "content": chunk_text,  # MeiliSearchìš© content í•„ë“œ
                                        "file_hash": file_hash  # ì¤‘ë³µ ê²€ì‚¬ìš© í•´ì‹œ ì¶”ê°€
                                    }
                                }
                                chunk_documents.append(doc)

                            # 2. Meilisearchìš© ì „ì²´ ë¬¸ì„œ (í’€í…ìŠ¤íŠ¸ ê²€ìƒ‰ìš©)
                            full_document = {
                                "id": document_id,  # ì „ì²´ ë¬¸ì„œëŠ” document_idë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                                "document_id": document_id,
                                "content": markdown_content,  # ì „ì²´ ë§ˆí¬ë‹¤ìš´ ë‚´ìš©
                                "title": file_info["filename"],
                                "file_name": file_info["filename"],
                                "file_type": file_type.value,
                                "file_size": file_info.get("size", 0),
                                "created_at": time.time(),
                                "chunk_count": len(chunks),  # ì²­í¬ ê°œìˆ˜ ì •ë³´
                                "metadata": {
                                    "filename": file_info["filename"],
                                    "file_type": file_type.value,
                                    "conversion_method": method,
                                    "created_at": time.time(),
                                    "document_id": document_id,
                                    "content": markdown_content,  # MeiliSearchìš© ì „ì²´ ë‚´ìš©
                                    "file_hash": file_hash,  # ì¤‘ë³µ ê²€ì‚¬ìš© í•´ì‹œ ì¶”ê°€
                                    "chunk_count": len(chunks)
                                }
                            }

                            # Store in unified search service (hybrid approach)
                            logger.info(f"ðŸ’¾ í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ë¬¸ì„œ ì €ìž¥ ì‹œìž‘: {len(chunk_documents)}ê°œ ì²­í¬ + 1ê°œ ì „ì²´ ë¬¸ì„œ")
                            unified_success = await unified_search_service.store_documents(
                                documents=chunk_documents,
                                full_document=full_document
                            )

                            if unified_success:
                                logger.info(f"âœ… í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì €ìž¥ ì™„ë£Œ: {document_id}")
                            else:
                                logger.error(f"âŒ í†µí•© ê²€ìƒ‰ ì„œë¹„ìŠ¤ ì €ìž¥ ì‹¤íŒ¨: {document_id}")
                    else:
                        logger.warning(f"âš ï¸ íŒŒì¼ í•´ì‹œ ì—†ìŒ - ë²¡í„° DB ì¤‘ë³µ ê²€ì‚¬ ìŠ¤í‚µ")

                except Exception as e:
                    logger.error(f"Error storing chunks in unified search service: {str(e)}")
                    # Don't fail the whole process if unified search fails
            
            logger.info(f"Document processed successfully: {file_info['filename']} -> {document_id}")
            
            return {
                "success": True,
                "file_id": file_id,
                "document_id": document_id,
                "filename": file_info["filename"],
                "conversion_method": method,
                "conversion_time": conversion_result["conversion_time"],
                "markdown_content": markdown_content,
                "markdown_length": len(markdown_content),
                "total_chunks": len(chunks),
                "chunks": chunks,
                "embeddings_generated": embeddings_generated,
                "processing_time": time.time() - start_time
            }
            
        except Exception as e:
            logger.error(f"Error processing document {file_id}: {str(e)}")
            return {
                "success": False,
                "error": f"Processing failed: {str(e)}",
                "processing_time": time.time() - start_time
            }
    
    def get_document(self, document_id: str) -> Optional[Dict]:
        """Get processed document by ID."""
        from services.database_service import database_service
        return database_service.get_document(document_id)

    def list_documents(self, page: int = 1, page_size: int = 100) -> Dict:
        """List all processed documents with pagination."""
        from services.database_service import database_service
        return database_service.list_documents(page, page_size)


# Global service instance
document_processing_service = DocumentProcessingService()
