#!/usr/bin/env python3
"""Test text chunking functionality."""

import requests
import json
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()


def test_chunking_api():
    """Test KURE API chunking functionality."""
    
    print("ğŸ§ª Testing KURE API Text Chunking")
    print("=" * 60)
    
    # Get configuration from environment
    api_key = os.getenv("API_KEY", "sk-kure-v1-test-key-12345")
    base_url = os.getenv("BASE_URL", "http://localhost:8000")
    
    print(f"ğŸ”‘ Using API Key: {api_key[:20]}...")
    print(f"ğŸŒ Base URL: {base_url}")
    
    # Headers with Bearer token
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    # Test 1: Korean text chunking with sentence strategy
    print("\n1. ğŸ“ Testing Korean text chunking (sentence strategy)...")
    try:
        korean_text = """
        ì•ˆë…•í•˜ì„¸ìš”, ì´ê²ƒì€ í•œêµ­ì–´ í…ìŠ¤íŠ¸ ì²­í‚¹ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. 
        KURE ëª¨ë¸ì€ í•œêµ­ì–´ ì„ë² ë”©ì— íŠ¹í™”ëœ ëª¨ë¸ì…ë‹ˆë‹¤. 
        ì´ í…ìŠ¤íŠ¸ëŠ” ì—¬ëŸ¬ ë¬¸ì¥ìœ¼ë¡œ êµ¬ì„±ë˜ì–´ ìˆìœ¼ë©°, ê° ë¬¸ì¥ì€ ì˜ë¯¸ ìˆëŠ” ë‹¨ìœ„ë¡œ ë¶„í• ë  ê²ƒì…ë‹ˆë‹¤.
        í…ìŠ¤íŠ¸ ì²­í‚¹ì€ ê¸´ ë¬¸ì„œë¥¼ ì²˜ë¦¬í•  ë•Œ ë§¤ìš° ìœ ìš©í•œ ê¸°ëŠ¥ì…ë‹ˆë‹¤.
        RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì—ì„œë„ ìì£¼ ì‚¬ìš©ë©ë‹ˆë‹¤.
        ì´ì œ ì´ í…ìŠ¤íŠ¸ê°€ ì–´ë–»ê²Œ ì²­í¬ë¡œ ë‚˜ë‰˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
        """
        
        payload = {
            "text": korean_text.strip(),
            "strategy": "sentence",
            "chunk_size": 100,
            "overlap": 20,
            "language": "ko"
        }
        
        response = requests.post(f"{base_url}/v1/chunk", json=payload, headers=headers)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Total chunks: {data['total_chunks']}")
            print(f"âœ… Strategy used: {data['strategy']}")
            print(f"âœ… Original length: {data['original_length']} chars")
            print(f"âœ… Total tokens: {data['total_tokens']}")
            
            for i, chunk in enumerate(data['data'][:3]):  # Show first 3 chunks
                print(f"  Chunk {i}: {chunk['token_count']} tokens, chars {chunk['start_char']}-{chunk['end_char']}")
                print(f"    Text: {chunk['text'][:100]}...")
        else:
            print(f"âŒ Korean chunking failed: {response.text}")
            
    except Exception as e:
        print(f"âŒ Korean chunking error: {e}")
    
    # Test 2: English text chunking with recursive strategy
    print("\n2. ğŸ”¤ Testing English text chunking (recursive strategy)...")
    try:
        english_text = """
        This is an English text chunking test for the KURE API. 
        The KURE model is specialized for Korean embeddings, but it should also handle English text well.
        Text chunking is a crucial feature for processing long documents in natural language processing.
        It helps break down large texts into manageable pieces that can be processed by language models.
        This functionality is particularly useful in RAG systems where documents need to be split into chunks.
        Each chunk should maintain semantic coherence while staying within token limits.
        """
        
        payload = {
            "text": english_text.strip(),
            "strategy": "recursive",
            "chunk_size": 80,
            "overlap": 15,
            "language": "en"
        }
        
        response = requests.post(f"{base_url}/v1/chunk", json=payload, headers=headers)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Total chunks: {data['total_chunks']}")
            print(f"âœ… Strategy used: {data['strategy']}")
            print(f"âœ… Original length: {data['original_length']} chars")
            print(f"âœ… Total tokens: {data['total_tokens']}")
            
            for i, chunk in enumerate(data['data'][:3]):  # Show first 3 chunks
                print(f"  Chunk {i}: {chunk['token_count']} tokens, chars {chunk['start_char']}-{chunk['end_char']}")
                print(f"    Text: {chunk['text'][:100]}...")
        else:
            print(f"âŒ English chunking failed: {response.text}")
            
    except Exception as e:
        print(f"âŒ English chunking error: {e}")
    
    # Test 3: Mixed language text with token strategy
    print("\n3. ğŸŒ Testing mixed language text (token strategy)...")
    try:
        mixed_text = """
        This is a mixed language test. ì´ê²ƒì€ í˜¼í•© ì–¸ì–´ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        We will test how the chunking algorithm handles both English and Korean text.
        í•œêµ­ì–´ì™€ ì˜ì–´ê°€ ì„ì¸ í…ìŠ¤íŠ¸ì—ì„œ ì²­í‚¹ì´ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€ í™•ì¸í•´ë³´ê² ìŠµë‹ˆë‹¤.
        The auto-detection feature should identify the primary language.
        ìë™ ì–¸ì–´ ê°ì§€ ê¸°ëŠ¥ì´ ì£¼ìš” ì–¸ì–´ë¥¼ ì‹ë³„í•´ì•¼ í•©ë‹ˆë‹¤.
        """
        
        payload = {
            "text": mixed_text.strip(),
            "strategy": "token",
            "chunk_size": 60,
            "overlap": 10,
            "language": "auto"
        }
        
        response = requests.post(f"{base_url}/v1/chunk", json=payload, headers=headers)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Total chunks: {data['total_chunks']}")
            print(f"âœ… Strategy used: {data['strategy']}")
            print(f"âœ… Original length: {data['original_length']} chars")
            print(f"âœ… Total tokens: {data['total_tokens']}")
            
            for i, chunk in enumerate(data['data']):
                print(f"  Chunk {i}: {chunk['token_count']} tokens, chars {chunk['start_char']}-{chunk['end_char']}")
                print(f"    Text: {chunk['text'][:80]}...")
        else:
            print(f"âŒ Mixed language chunking failed: {response.text}")
            
    except Exception as e:
        print(f"âŒ Mixed language chunking error: {e}")
    
    # Test 4: Error handling - invalid strategy
    print("\n4. âš ï¸ Testing error handling (invalid strategy)...")
    try:
        payload = {
            "text": "Test text for error handling",
            "strategy": "invalid_strategy",
            "chunk_size": 100,
            "overlap": 10,
            "language": "auto"
        }
        
        response = requests.post(f"{base_url}/v1/chunk", json=payload, headers=headers)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 422:
            print("âœ… Validation error properly handled")
            error_data = response.json()
            print(f"âœ… Error details: {error_data}")
        else:
            print(f"âŒ Unexpected response: {response.text}")
            
    except Exception as e:
        print(f"âŒ Error handling test failed: {e}")
    
    # Test 5: Large chunk size
    print("\n5. ğŸ“ Testing large chunk size...")
    try:
        large_text = "ì´ê²ƒì€ í° ì²­í¬ í¬ê¸° í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤. " * 50  # Repeat to make it longer
        
        payload = {
            "text": large_text,
            "strategy": "sentence",
            "chunk_size": 1000,
            "overlap": 50,
            "language": "ko"
        }
        
        response = requests.post(f"{base_url}/v1/chunk", json=payload, headers=headers)
        print(f"Status: {response.status_code}")
        
        if response.status_code == 200:
            data = response.json()
            print(f"âœ… Total chunks: {data['total_chunks']}")
            print(f"âœ… Large text handled successfully")
            print(f"âœ… Total tokens: {data['total_tokens']}")
        else:
            print(f"âŒ Large chunk test failed: {response.text}")
            
    except Exception as e:
        print(f"âŒ Large chunk test error: {e}")
    
    print("\nğŸ‰ Text Chunking Test Completed!")
    return True


if __name__ == "__main__":
    test_chunking_api()
