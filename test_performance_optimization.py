#!/usr/bin/env python3
"""
Test script for performance optimization functionality.
"""

import asyncio
import time
import logging
import random
import string
from pathlib import Path

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Import services
from services.file_upload_service import file_upload_service
from services.database_service import database_service


class MockUploadFile:
    """Mock UploadFile for testing."""
    
    def __init__(self, filename: str, content: bytes, content_type: str = "text/plain"):
        self.filename = filename
        self.content = content
        self.content_type = content_type
        self.size = len(content)
    
    async def read(self) -> bytes:
        return self.content


def generate_test_content(size_mb: int) -> bytes:
    """Generate test content of specified size."""
    size_bytes = size_mb * 1024 * 1024
    # Generate random content
    content = ''.join(random.choices(string.ascii_letters + string.digits + ' \n', k=size_bytes))
    return content.encode('utf-8')


async def test_chunked_hash_calculation():
    """Test chunked hash calculation for large files."""
    
    logger.info("ğŸ§ª ì²­í¬ ë‹¨ìœ„ í•´ì‹œ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    
    # Test with different file sizes
    test_sizes = [1, 5, 15, 25]  # MB
    
    for size_mb in test_sizes:
        logger.info(f"í…ŒìŠ¤íŠ¸ íŒŒì¼ í¬ê¸°: {size_mb}MB")
        
        content = generate_test_content(size_mb)
        
        # Measure hash calculation time
        start_time = time.time()
        file_hash = file_upload_service._calculate_file_hash(content)
        calculation_time = time.time() - start_time
        
        logger.info(f"  í•´ì‹œ: {file_hash[:16]}...")
        logger.info(f"  ê³„ì‚° ì‹œê°„: {calculation_time:.3f}ì´ˆ")
        logger.info(f"  ì²˜ë¦¬ ì†ë„: {size_mb / calculation_time:.1f}MB/s")
        
        # Verify hash is valid
        assert len(file_hash) == 64 or file_hash.startswith("md5_") or file_hash.startswith("fallback_")
    
    logger.info("âœ… ì²­í¬ ë‹¨ìœ„ í•´ì‹œ ê³„ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼")


async def test_async_hash_calculation():
    """Test asynchronous hash calculation."""
    
    logger.info("ğŸ§ª ë¹„ë™ê¸° í•´ì‹œ ê³„ì‚° í…ŒìŠ¤íŠ¸")
    
    # Generate large test content (60MB to trigger async processing)
    large_content = generate_test_content(60)
    
    # Test async hash calculation
    start_time = time.time()
    file_hash = await file_upload_service._calculate_file_hash_async(large_content)
    async_time = time.time() - start_time
    
    logger.info(f"ë¹„ë™ê¸° í•´ì‹œ: {file_hash[:16]}...")
    logger.info(f"ë¹„ë™ê¸° ê³„ì‚° ì‹œê°„: {async_time:.3f}ì´ˆ")
    
    # Compare with sync calculation
    start_time = time.time()
    sync_hash = file_upload_service._calculate_file_hash(large_content)
    sync_time = time.time() - start_time
    
    logger.info(f"ë™ê¸° í•´ì‹œ: {sync_hash[:16]}...")
    logger.info(f"ë™ê¸° ê³„ì‚° ì‹œê°„: {sync_time:.3f}ì´ˆ")
    
    # Hashes should be the same
    assert file_hash == sync_hash
    
    logger.info("âœ… ë¹„ë™ê¸° í•´ì‹œ ê³„ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼")


async def test_cache_performance():
    """Test hash cache performance."""
    
    logger.info("ğŸ§ª ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    # Clear cache first
    file_upload_service.clear_cache()
    
    # Generate test data
    test_hash = "test_hash_123456789abcdef"
    test_file_info = {
        "file_id": "test_id",
        "filename": "test_file.txt",
        "file_type": "txt",
        "file_size": 1024
    }
    
    # Test cache miss
    start_time = time.time()
    cached_result = file_upload_service._check_hash_cache(test_hash)
    cache_miss_time = time.time() - start_time
    
    assert cached_result is None
    logger.info(f"ìºì‹œ ë¯¸ìŠ¤ ì‹œê°„: {cache_miss_time * 1000:.3f}ms")
    
    # Update cache
    file_upload_service._update_hash_cache(test_hash, test_file_info)
    
    # Test cache hit
    start_time = time.time()
    cached_result = file_upload_service._check_hash_cache(test_hash)
    cache_hit_time = time.time() - start_time
    
    assert cached_result == test_file_info
    logger.info(f"ìºì‹œ íˆíŠ¸ ì‹œê°„: {cache_hit_time * 1000:.3f}ms")
    
    # Cache hit should be much faster
    assert cache_hit_time < cache_miss_time
    
    logger.info("âœ… ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")


async def test_memory_efficiency():
    """Test memory efficiency for large files."""
    
    logger.info("ğŸ§ª ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸")
    
    # Test with large file (20MB)
    large_content = generate_test_content(20)
    mock_file = MockUploadFile("large_test.txt", large_content)
    
    # Monitor memory usage during upload
    import psutil
    import os
    
    process = psutil.Process(os.getpid())
    memory_before = process.memory_info().rss / 1024 / 1024  # MB
    
    # Upload file
    start_time = time.time()
    result = await file_upload_service.upload_file(mock_file)
    upload_time = time.time() - start_time
    
    memory_after = process.memory_info().rss / 1024 / 1024  # MB
    memory_used = memory_after - memory_before
    
    logger.info(f"ì—…ë¡œë“œ ì„±ê³µ: {result['success']}")
    logger.info(f"ì—…ë¡œë“œ ì‹œê°„: {upload_time:.3f}ì´ˆ")
    logger.info(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_used:.1f}MB")
    logger.info(f"íŒŒì¼ í¬ê¸° ëŒ€ë¹„ ë©”ëª¨ë¦¬ ë¹„ìœ¨: {memory_used / 20 * 100:.1f}%")
    
    # Memory usage should be reasonable (less than 2x file size)
    assert memory_used < 40  # Less than 40MB for 20MB file
    
    logger.info("âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì„± í…ŒìŠ¤íŠ¸ í†µê³¼")


def test_database_performance():
    """Test database performance optimizations."""
    
    logger.info("ğŸ§ª ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    # Test performance stats
    start_time = time.time()
    perf_stats = database_service.get_performance_stats()
    stats_time = time.time() - start_time
    
    logger.info(f"ì„±ëŠ¥ í†µê³„ ì¡°íšŒ ì‹œê°„: {stats_time:.3f}ì´ˆ")
    logger.info(f"í…Œì´ë¸” í†µê³„: {perf_stats.get('table_stats', {})}")
    logger.info(f"ì¸ë±ìŠ¤ ìˆ˜: {perf_stats.get('index_count', 0)}")
    logger.info(f"DB í¬ê¸°: {perf_stats.get('database_size_mb', 0)}MB")
    
    # Test batch operations
    test_pairs = [
        ("hash1", 5),
        ("hash2", 3),
        ("hash3", 7)
    ]
    
    start_time = time.time()
    batch_result = database_service.batch_update_upload_counts(test_pairs)
    batch_time = time.time() - start_time
    
    logger.info(f"ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì„±ê³µ: {batch_result}")
    logger.info(f"ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹œê°„: {batch_time:.3f}ì´ˆ")
    
    # Performance should be reasonable
    assert stats_time < 1.0  # Less than 1 second
    assert batch_time < 0.5  # Less than 0.5 seconds
    
    logger.info("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")


async def test_concurrent_uploads():
    """Test concurrent upload performance."""
    
    logger.info("ğŸ§ª ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸")
    
    # Generate multiple test files
    test_files = []
    for i in range(5):
        content = generate_test_content(2)  # 2MB each
        mock_file = MockUploadFile(f"concurrent_test_{i}.txt", content)
        test_files.append(mock_file)
    
    # Test sequential uploads
    start_time = time.time()
    sequential_results = []
    for mock_file in test_files:
        result = await file_upload_service.upload_file(mock_file)
        sequential_results.append(result)
    sequential_time = time.time() - start_time
    
    logger.info(f"ìˆœì°¨ ì—…ë¡œë“œ ì‹œê°„: {sequential_time:.3f}ì´ˆ")
    
    # Clear cache and prepare for concurrent test
    file_upload_service.clear_cache()
    
    # Test concurrent uploads
    start_time = time.time()
    concurrent_tasks = [file_upload_service.upload_file(mock_file) for mock_file in test_files]
    concurrent_results = await asyncio.gather(*concurrent_tasks)
    concurrent_time = time.time() - start_time
    
    logger.info(f"ë™ì‹œ ì—…ë¡œë“œ ì‹œê°„: {concurrent_time:.3f}ì´ˆ")
    logger.info(f"ì„±ëŠ¥ í–¥ìƒ: {sequential_time / concurrent_time:.1f}x")
    
    # All uploads should succeed
    assert all(result['success'] for result in concurrent_results)
    
    # Concurrent should be faster (or at least not much slower)
    assert concurrent_time <= sequential_time * 1.2  # Allow 20% tolerance
    
    logger.info("âœ… ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼")


async def main():
    """Run all performance tests."""
    
    logger.info("ğŸš€ ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    tests = [
        ("ì²­í¬ ë‹¨ìœ„ í•´ì‹œ ê³„ì‚°", test_chunked_hash_calculation),
        ("ë¹„ë™ê¸° í•´ì‹œ ê³„ì‚°", test_async_hash_calculation),
        ("ìºì‹œ ì„±ëŠ¥", test_cache_performance),
        ("ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±", test_memory_efficiency),
        ("ë°ì´í„°ë² ì´ìŠ¤ ì„±ëŠ¥", test_database_performance),
        ("ë™ì‹œ ì—…ë¡œë“œ ì„±ëŠ¥", test_concurrent_uploads)
    ]
    
    results = []
    
    for test_name, test_func in tests:
        try:
            logger.info(f"\n{'='*50}")
            logger.info(f"í…ŒìŠ¤íŠ¸: {test_name}")
            logger.info(f"{'='*50}")
            
            if asyncio.iscoroutinefunction(test_func):
                await test_func()
            else:
                test_func()
            
            results.append((test_name, True))
            logger.info(f"âœ… {test_name} í†µê³¼")
            
        except Exception as e:
            logger.error(f"âŒ {test_name} ì‹¤íŒ¨: {str(e)}")
            results.append((test_name, False))
    
    # Summary
    logger.info(f"\n{'='*50}")
    logger.info("ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
    logger.info(f"{'='*50}")
    
    passed = sum(1 for _, result in results if result)
    total = len(results)
    
    for test_name, result in results:
        status = "âœ… í†µê³¼" if result else "âŒ ì‹¤íŒ¨"
        logger.info(f"{test_name}: {status}")
    
    logger.info(f"\nì´ {total}ê°œ í…ŒìŠ¤íŠ¸ ì¤‘ {passed}ê°œ í†µê³¼ ({passed/total*100:.1f}%)")
    
    if passed == total:
        logger.info("ğŸ‰ ëª¨ë“  ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        logger.error(f"âš ï¸ {total - passed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False


if __name__ == "__main__":
    success = asyncio.run(main())
    exit(0 if success else 1)
